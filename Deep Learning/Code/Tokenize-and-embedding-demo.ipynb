{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf1739e0",
   "metadata": {
    "id": "cf1739e0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nThis code will walk through the core steps: taking a small text corpus, automatically building a vocabulary from it,\\nand then using that vocabulary to convert text to numbers and back again.\\n\\nHow This Code Relates to the Tokenization Guide provided to you as a pdf\\n\\n    Corpus: The corpus list is our small-scale version of the text data mentioned in Step 1.\\n\\n    Strategy & Training: tf.keras.layers.TextVectorization acts as our word-based tokenizer. The .adapt(corpus) method performs Step 2 and 3,\\n     automatically learning the vocabulary from the data.\\n\\n    Special Tokens: When you inspect the vocabulary, you'll see it automatically includes '' (for padding, which maps to 0) and [UNK]\\n    (for unknown words, which maps to 1). This corresponds to Step 4.\\n\\n    Final Tokenizer: The text_vectorizer object is our finalized tokenizer, ready to perform the encoding and decoding tasks described in Step 5.\\n\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "This code will walk through the core steps: taking a small text corpus, automatically building a vocabulary from it,\n",
    "and then using that vocabulary to convert text to numbers and back again.\n",
    "\n",
    "How This Code Relates to the Tokenization Guide provided to you as a pdf\n",
    "\n",
    "    Corpus: The corpus list is our small-scale version of the text data mentioned in Step 1.\n",
    "\n",
    "    Strategy & Training: tf.keras.layers.TextVectorization acts as our word-based tokenizer. The .adapt(corpus) method performs Step 2 and 3,\n",
    "     automatically learning the vocabulary from the data.\n",
    "\n",
    "    Special Tokens: When you inspect the vocabulary, you'll see it automatically includes '' (for padding, which maps to 0) and [UNK]\n",
    "    (for unknown words, which maps to 1). This corresponds to Step 4.\n",
    "\n",
    "    Final Tokenizer: The text_vectorizer object is our finalized tokenizer, ready to perform the encoding and decoding tasks described in Step 5.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "780a4497",
   "metadata": {
    "id": "780a4497",
    "outputId": "536112e7-d97e-495d-9404-8986a93e975c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training the tokenizer... ---\n",
      "Vocabulary built successfully!\n",
      "------------------------------\n",
      "Vocabulary Size: 14\n",
      "Learned Vocabulary (Word -> ID):\n",
      "0: \n",
      "1: [UNK]\n",
      "2: the\n",
      "3: dog\n",
      "4: cat\n",
      "5: sat\n",
      "6: on\n",
      "7: my\n",
      "8: mat\n",
      "9: homework\n",
      "------------------------------\n",
      "Original sentence: 'the dog and cat are good friends'\n",
      "Encoded sequence: [[ 2  3 13  4 12  1 10  0]]\n",
      "------------------------------\n",
      "Original sentence: 'the cat sat on the mat'\n",
      "Encoded sequence: [[2 4 5 6 2 8 0 0]]\n",
      "------------------------------\n",
      "Original sentence: 'the dog ate my homework'\n",
      "Encoded sequence: [[ 2  3 11  7  9  0  0  0]]\n",
      "------------------------------\n",
      "Original sentence: 'the cat and the dog are friends'\n",
      "Encoded sequence: [[ 2  4 13  2  3 12 10  0]]\n",
      "------------------------------\n",
      "Sequence to decode: [ 2  4 13  2  3 12 10  0]\n",
      "Decoded sentence: 'the cat and the dog are friends'\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# --- Step 1: Gather and Prepare a Corpus ---\n",
    "# For this demo, our \"corpus\" is just a small list of sentences.\n",
    "# In a real project, this would be thousands or millions of sentences from your dataset.\n",
    "corpus = [\n",
    "    \"the cat sat on the mat\",\n",
    "    \"the dog ate my homework\",\n",
    "    \"the cat and the dog are friends\"\n",
    "]\n",
    "\n",
    "# --- Step 2 & 3: Choose a Strategy and Build the Vocabulary ---\n",
    "# We will use a simple word-based strategy. The TextVectorization layer is a\n",
    "# convenient tool that handles the entire vocabulary creation process.\n",
    "\n",
    "# Define the maximum number of words to include in the vocabulary.\n",
    "# The layer will automatically pick the most frequent words.\n",
    "vocab_size = 15\n",
    "\n",
    "# Create the TextVectorization layer. This is our tokenizer.\n",
    "# It will handle normalization (like lowercasing) and splitting text into words.\n",
    "# `output_sequence_length` pads or truncates sentences to a fixed length.\n",
    "text_vectorizer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_sequence_length=8\n",
    ")\n",
    "\n",
    "# Train the tokenizer on our corpus to build the vocabulary.\n",
    "# The .adapt() method reads the corpus, counts word frequencies,\n",
    "# and creates the mapping from words to integer IDs.\n",
    "print(\"--- Training the tokenizer... ---\")\n",
    "text_vectorizer.adapt(corpus)\n",
    "print(\"Vocabulary built successfully!\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "\n",
    "# --- Step 4: Inspect the Vocabulary (and Special Tokens) ---\n",
    "# We can now view the vocabulary that the layer has learned.\n",
    "# The layer automatically handles adding special tokens like [UNK] for\n",
    "# out-of-vocabulary words and '0' for padding.\n",
    "vocabulary = text_vectorizer.get_vocabulary()\n",
    "print(f\"Vocabulary Size: {len(vocabulary)}\")\n",
    "print(\"Learned Vocabulary (Word -> ID):\")\n",
    "# Print the first 10 words and their corresponding IDs (indices).\n",
    "for i, word in enumerate(vocabulary[:10]):\n",
    "    print(f\"{i}: {word}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "\n",
    "# --- Step 5: Finalize and Use the Tokenizer ---\n",
    "# Our tokenizer is now ready to use!\n",
    "\n",
    "# --- Demonstration: Encoding (Text to Numbers) ---\n",
    "sentence_to_encode = \"the dog and cat are good friends\"\n",
    "print(f\"Original sentence: '{sentence_to_encode}'\")\n",
    "\n",
    "# Use the trained tokenizer to convert the sentence into a sequence of integer IDs.\n",
    "# Note that 'good' is not in our original corpus, so it will be mapped to the [UNK] token (ID 1).\n",
    "encoded_sentence = text_vectorizer([sentence_to_encode])\n",
    "print(f\"Encoded sequence: {encoded_sentence.numpy()}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "\n",
    "sentence_to_encode = \"the cat sat on the mat\"\n",
    "print(f\"Original sentence: '{sentence_to_encode}'\")\n",
    "\n",
    "# Use the trained tokenizer to convert the sentence into a sequence of integer IDs.\n",
    "# Note that 'good' is not in our original corpus, so it will be mapped to the [UNK] token (ID 1).\n",
    "encoded_sentence = text_vectorizer([sentence_to_encode])\n",
    "print(f\"Encoded sequence: {encoded_sentence.numpy()}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "\n",
    "sentence_to_encode = \"the dog ate my homework\"\n",
    "print(f\"Original sentence: '{sentence_to_encode}'\")\n",
    "\n",
    "# Use the trained tokenizer to convert the sentence into a sequence of integer IDs.\n",
    "# Note that 'good' is not in our original corpus, so it will be mapped to the [UNK] token (ID 1).\n",
    "encoded_sentence = text_vectorizer([sentence_to_encode])\n",
    "print(f\"Encoded sequence: {encoded_sentence.numpy()}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "\n",
    "sentence_to_encode = \"the cat and the dog are friends\"\n",
    "print(f\"Original sentence: '{sentence_to_encode}'\")\n",
    "\n",
    "# Use the trained tokenizer to convert the sentence into a sequence of integer IDs.\n",
    "# Note that 'good' is not in our original corpus, so it will be mapped to the [UNK] token (ID 1).\n",
    "encoded_sentence = text_vectorizer([sentence_to_encode])\n",
    "print(f\"Encoded sequence: {encoded_sentence.numpy()}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "\n",
    "\n",
    "# --- Demonstration: Decoding (Numbers to Text) ---\n",
    "# We can also build a simple decoder to convert the numbers back to text.\n",
    "# First, create a reverse mapping from ID to word.\n",
    "id_to_word_map = {i: word for i, word in enumerate(vocabulary)}\n",
    "\n",
    "encoded_sequence_to_decode = encoded_sentence.numpy()[0]\n",
    "print(f\"Sequence to decode: {encoded_sequence_to_decode}\")\n",
    "\n",
    "# Decode the sequence by looking up each ID in our map.\n",
    "# We'll ignore padding tokens (ID 0).\n",
    "decoded_sentence = ' '.join(id_to_word_map[i] for i in encoded_sequence_to_decode if i > 0)\n",
    "print(f\"Decoded sentence: '{decoded_sentence}'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37944a60",
   "metadata": {
    "id": "37944a60"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nObtaining word embeddings using the Keras `Embedding` layer is like using a **smart dictionary**.\\nInstead of looking up a word to get its definition, look up a word\\'s unique ID number to get a dense vector of\\nnumbers that represents its meaning.\\n\\nThe `Embedding` layer is essentially a **lookup table** that you create and train.\\nIt stores one vector for every word in your vocabulary.\\n\\nThe Three-Step Process\\n\\n1. Step 1: Prepare Your Data (Text to Integers)\\n\\nFirst, convert raw text into sequences of integer IDs. Each unique word in your entire dataset is assigned\\na unique integer.\\n\\nFor example, the sentence:\\n`\"the cat sat on the mat\"`\\n\\nBecomes a sequence of integers:\\n`[2 4 5 6 2 8 0 0]`\\n\\n2. Step 2: Define the `Embedding` Layer 📖\\n\\nWhen you create the layer, you define two key parameters:\\n\\n1.  input_dim: This is the size of your vocabulary (the total number of unique words). In our example, it would be 14 (12 words + 1 for a \\'0\\' padding token + 1 for [UNK]).\\n2.  output_dim: This is the size of the dense vector you want for each word. This is a hyperparameter you choose. A common size is 128, 256, or 512.\\n\\n\\n```python\\n# Let\\'s say we want a 20-dimensional vector for each word.\\nembedding_layer = tf.keras.layers.Embedding(input_dim=14, output_dim=20)\\n```\\n\\nBehind the scenes, Keras creates a simple but powerful weight matrix (our \"lookup table\") of shape `(input_dim, output_dim)`.\\nFor our example, this would be a (14, 20) matrix. Initially, this matrix is filled with small random numbers, then gets adjusted during training.\\n\\n3. Step 3: The Lookup Operation 🔍\\n\\nWhen you pass your integer sequence [2 4 5 6 2 8 0 0] into the layer, it performs a direct lookup.\\n\\n  * For the integer 2, it grabs the corresponding vector of the matrix.\\n  * For the integer 4, it grabs the corresponding vector of the matrix.\\n  * And so on...\\n\\nThe output is a new sequence where each integer ID has been replaced by its corresponding dense vector from the lookup table.\\n\\nThe Magic: The most important part is that these vectors are LEARNED during training.\\nThrough backpropagation, the model adjusts the values in these vectors. As a result, words that are used in similar contexts\\n(e.g., \"cat\" and \"dog,\" or \"king\" and \"queen\") will end up having similar-looking vectors.\\nThis is how the model captures the semantic meaning of words.\\n\\n\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Obtaining word embeddings using the Keras `Embedding` layer is like using a **smart dictionary**.\n",
    "Instead of looking up a word to get its definition, look up a word's unique ID number to get a dense vector of\n",
    "numbers that represents its meaning.\n",
    "\n",
    "The `Embedding` layer is essentially a **lookup table** that you create and train.\n",
    "It stores one vector for every word in your vocabulary.\n",
    "\n",
    "The Three-Step Process\n",
    "\n",
    "1. Step 1: Prepare Your Data (Text to Integers)\n",
    "\n",
    "First, convert raw text into sequences of integer IDs. Each unique word in your entire dataset is assigned\n",
    "a unique integer.\n",
    "\n",
    "For example, the sentence:\n",
    "`\"the cat sat on the mat\"`\n",
    "\n",
    "Becomes a sequence of integers:\n",
    "`[2 4 5 6 2 8 0 0]`\n",
    "\n",
    "2. Step 2: Define the `Embedding` Layer 📖\n",
    "\n",
    "When you create the layer, you define two key parameters:\n",
    "\n",
    "1.  input_dim: This is the size of your vocabulary (the total number of unique words). In our example, it would be 14 (12 words + 1 for a '0' padding token + 1 for [UNK]).\n",
    "2.  output_dim: This is the size of the dense vector you want for each word. This is a hyperparameter you choose. A common size is 128, 256, or 512.\n",
    "\n",
    "\n",
    "```python\n",
    "# Let's say we want a 20-dimensional vector for each word.\n",
    "embedding_layer = tf.keras.layers.Embedding(input_dim=14, output_dim=20)\n",
    "```\n",
    "\n",
    "Behind the scenes, Keras creates a simple but powerful weight matrix (our \"lookup table\") of shape `(input_dim, output_dim)`.\n",
    "For our example, this would be a (14, 20) matrix. Initially, this matrix is filled with small random numbers, then gets adjusted during training.\n",
    "\n",
    "3. Step 3: The Lookup Operation 🔍\n",
    "\n",
    "When you pass your integer sequence [2 4 5 6 2 8 0 0] into the layer, it performs a direct lookup.\n",
    "\n",
    "  * For the integer 2, it grabs the corresponding vector of the matrix.\n",
    "  * For the integer 4, it grabs the corresponding vector of the matrix.\n",
    "  * And so on...\n",
    "\n",
    "The output is a new sequence where each integer ID has been replaced by its corresponding dense vector from the lookup table.\n",
    "\n",
    "The Magic: The most important part is that these vectors are LEARNED during training.\n",
    "Through backpropagation, the model adjusts the values in these vectors. As a result, words that are used in similar contexts\n",
    "(e.g., \"cat\" and \"dog,\" or \"king\" and \"queen\") will end up having similar-looking vectors.\n",
    "This is how the model captures the semantic meaning of words.\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40c3a58f",
   "metadata": {
    "id": "40c3a58f",
    "outputId": "8775c9bd-700d-42ea-8fe5-5077898963f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Model Architecture ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_4\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_4\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ text_vectorization_5            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TextVectorization</span>)             │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)          │           <span style=\"color: #00af00; text-decoration-color: #00af00\">280</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling1d_4      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ text_vectorization_5            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)              │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mTextVectorization\u001b[0m)             │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding_layer (\u001b[38;5;33mEmbedding\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m20\u001b[0m)          │           \u001b[38;5;34m280\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling1d_4      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m21\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">301</span> (1.18 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m301\u001b[0m (1.18 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">301</span> (1.18 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m301\u001b[0m (1.18 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "<Embedding name=embedding_layer, built=True>\n",
      "\n",
      "--- Initial Embedding Weights (before training) ---\n",
      "Shape of embedding matrix: (14, 20)\n",
      "(Rows = Vocabulary Size, Columns = Embedding Dimension)\n",
      "\n",
      "Embedding vector for a specific word:\n",
      "Word: 'dog' (ID: 3)\n",
      "Initial Vector: [-0.0102998  -0.04280004 -0.02198629 -0.03258574 -0.0298155   0.01752689\n",
      "  0.02663679 -0.00136243 -0.04051492 -0.01391686 -0.01206525  0.01693368\n",
      "  0.02741503  0.00681318  0.00703589 -0.03886237 -0.03923808 -0.04380064\n",
      " -0.04333847  0.03581171]\n",
      "\n",
      "After training on a real task, this vector would capture the 'meaning' of the word 'dog'.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# --- Step 1: Reuse the Tokenizer Developed Previously ---\n",
    "# We'll start with the same corpus and the trained TextVectorization layer\n",
    "# from the previous example.\n",
    "\n",
    "corpus = [\n",
    "    \"the cat sat on the mat\",\n",
    "    \"the dog ate my homework\",\n",
    "    \"the cat and the dog are friends\"\n",
    "]\n",
    "\n",
    "vocab_size = 15\n",
    "text_vectorizer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_sequence_length=8\n",
    ")\n",
    "# Train the tokenizer on our corpus to build the vocabulary.\n",
    "text_vectorizer.adapt(corpus)\n",
    "\n",
    "\n",
    "# --- Step 2: Build a Model to Learn Embeddings ---\n",
    "# We'll create a simple Keras model. The key is that the first layer is our\n",
    "# tokenizer, and the second is the Embedding layer.\n",
    "\n",
    "# Define the dimensionality of the word embeddings we want to learn.\n",
    "embedding_dim = 20\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    # 1. The TextVectorization layer: This layer takes raw text strings as input\n",
    "    #    and outputs integer sequences. It's the bridge from text to numbers.\n",
    "    text_vectorizer,\n",
    "\n",
    "    # 2. The Embedding layer: This layer takes the integer sequences and looks up\n",
    "    #    the corresponding embedding vector for each token. The `input_dim` must\n",
    "    #    match the vocabulary size from our tokenizer.\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=len(text_vectorizer.get_vocabulary()),\n",
    "        output_dim=embedding_dim,\n",
    "        name=\"embedding_layer\" # Give the layer a name to easily access it later\n",
    "    ),\n",
    "\n",
    "    # 3. A Pooling layer: To get a single vector representation for the whole\n",
    "    #    sentence, we average the embeddings of all words in the sequence.\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "\n",
    "    # 4. A final Dense layer: To make this a trainable model, we add a simple\n",
    "    #    output layer. For a real task, this would be your classification or\n",
    "    #    regression output.\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid') # Example for binary classification\n",
    "])\n",
    "\n",
    "# Compile the model to prepare it for training\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Explicitly build the model so we can inspect the weights\n",
    "# The input_shape (None,) indicates a batch of strings.\n",
    "model.build(input_shape=(None,))\n",
    "\n",
    "# Print the model summary to see the architecture\n",
    "print(\"--- Model Architecture ---\")\n",
    "model.summary()\n",
    "print(\"-\" * 30)\n",
    "\n",
    "\n",
    "# --- Step 3: Inspect the Learned Embeddings ---\n",
    "# Although we haven't trained the model on any data yet, the embedding layer\n",
    "# already has a randomly initialized weight matrix. After training, these weights\n",
    "# would contain the meaningful learned embeddings.\n",
    "\n",
    "# Get the embedding layer from the model by its name\n",
    "embedding_layer = model.get_layer('embedding_layer')\n",
    "\n",
    "print(embedding_layer)\n",
    "\n",
    "# The weights are a list, where the first element is the embedding matrix\n",
    "embedding_weights = embedding_layer.get_weights()[0]\n",
    "\n",
    "# Get the vocabulary from our tokenizer\n",
    "vocabulary = text_vectorizer.get_vocabulary()\n",
    "\n",
    "print(\"\\n--- Initial Embedding Weights (before training) ---\")\n",
    "print(f\"Shape of embedding matrix: {embedding_weights.shape}\")\n",
    "print(\"(Rows = Vocabulary Size, Columns = Embedding Dimension)\")\n",
    "print(\"\\nEmbedding vector for a specific word:\")\n",
    "# Let's find the ID for the word 'dog'\n",
    "word_to_find = 'dog'\n",
    "word_id = vocabulary.index(word_to_find)\n",
    "# The embedding vector is the row in the weight matrix corresponding to the word's ID\n",
    "word_vector = embedding_weights[word_id]\n",
    "print(f\"Word: '{word_to_find}' (ID: {word_id})\")\n",
    "print(f\"Initial Vector: {word_vector}\")\n",
    "print(\"\\nAfter training on a real task, this vector would capture the 'meaning' of the word 'dog'.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
