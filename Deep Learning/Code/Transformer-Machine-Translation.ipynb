{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb6d4ee4",
   "metadata": {},
   "source": [
    "The problem is to build and train a Neural Machine Translation (NMT) model from scratch. The goal is to implement the Transformer \n",
    "architecture to translate sentences from Portuguese to English, using the TED Talks dataset for training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da1400cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nStep 1: Import libraries\\nStep 2: Load the dataset from TensorFlow Datasets\\nStep 3: Define the pre-trained Tokenizers for both languages\\nStep 4: Define the functions for preparing batches for training and evaluation\\nStep 5: Define Positional Encoding to add the position information to the tokens \\nStep 6: Defines Keras layers that implement self attention, cross-attention. \\nStep 7: Defines a Keras layer that implements the \"Position-wise Feed-Forward Network\" described in the original \"Attention Is All You Need\" paper. \\nStep 8: Define the Encoder layer and Encoder model with multiple layers\\nStep 9: Define causal or masked attention layer and decoder layers, followed by the overall decoder model\\nStep 10: Define the transformer by combining the encoder and decoder models\\nStep 11: Define the los and accuracy measures\\nStep 12: Define the training hyper-parameters and do the training\\nStep 13: Develop the Translator class with the transformer to evaluate the model\\n '"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Step 1: Import libraries\n",
    "Step 2: Load the dataset from TensorFlow Datasets\n",
    "Step 3: Define the pre-trained Tokenizers for both languages\n",
    "Step 4: Define the functions for preparing batches for training and evaluation\n",
    "Step 5: Define Positional Encoding to add the position information to the tokens \n",
    "Step 6: Defines Keras layers that implement self attention, cross-attention. \n",
    "Step 7: Defines a Keras layer that implements the \"Position-wise Feed-Forward Network\" described in the original \"Attention Is All You Need\" paper. \n",
    "Step 8: Define the Encoder layer and Encoder model with multiple layers\n",
    "Step 9: Define causal or masked attention layer and decoder layers, followed by the overall decoder model\n",
    "Step 10: Define the transformer by combining the encoder and decoder models\n",
    "Step 11: Define the los and accuracy measures\n",
    "Step 12: Define the training hyper-parameters and do the training\n",
    "Step 13: Develop the Translator class with the transformer to evaluate the model\n",
    " '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45e2ea02",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow_datasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtfds\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow_text\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tensorflow_text'"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6654105",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Step 2: Load the dataset from TensorFlow Datasets\n",
    "Use TensorFlow Datasets to load the Portuguese-English translation datasetD Talks Open Translation Project. \n",
    "This dataset contains approximately 52,000 training, 1,200 validation and 1,800 test examples.\n",
    "\n",
    "'''\n",
    "\n",
    "# 'ted_hrlr_translate/pt_to_en' is the Portuguese-to-English translation dataset.\n",
    "# with_info=True provides metadata about the dataset.\n",
    "# as_supervised=True loads the data as (input, label) pairs.\n",
    "examples, metadata = tfds.load('ted_hrlr_translate/pt_to_en',\n",
    "                               with_info=True,\n",
    "                               as_supervised=True)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_examples, val_examples = examples['train'], examples['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14ea4fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Examples in Portuguese:\n",
      "e quando melhoramos a procura , tiramos a única vantagem da impressão , que é a serendipidade .\n",
      "mas e se estes fatores fossem ativos ?\n",
      "mas eles não tinham a curiosidade de me testar .\n",
      "\n",
      "> Examples in English:\n",
      "and when you improve searchability , you actually take away the one advantage of print , which is serendipity .\n",
      "but what if it were active ?\n",
      "but they did n't test for curiosity .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-31 19:36:40.309974: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "# CHECK: Print a few examples to see what the data looks like\n",
    "for pt_examples, en_examples in train_examples.batch(3).take(1):\n",
    "  print('> Examples in Portuguese:')\n",
    "  for pt in pt_examples.numpy():\n",
    "    print(pt.decode('utf-8'))\n",
    "  print()\n",
    "\n",
    "  print('> Examples in English:')\n",
    "  for en in en_examples.numpy():\n",
    "    print(en.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98db6cdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\nStep 3: Define the pre-trained Tokenizers for both languages\\nhttps://www.youtube.com/watch?v=L5CR-k2ROu4 : Tokenization in NLP: Basics to Advanced\\n\\nIts main purpose is to download and load a pre-trained tokenizer model from a public repository hosted by TensorFlow.\\n\\nA tokenizer is a fundamental tool in Natural Language Processing (NLP). It handles two key tasks:\\n\\n    Tokenization: It breaks down raw text (like a sentence) into smaller units called \"tokens.\" These tokens are then mapped to unique numerical IDs. \\n    Neural networks can only process numbers, so this step is essential to convert text into a format the model can understand.\\n\\n    Detokenization: It performs the reverse operation, converting the numerical output from the model back into human-readable text.\\n\\nThe process in this snippet happens in two main steps:\\n\\n    Downloading and Extracting the Model (tf.keras.utils.get_file): This function is a convenient utility that handles fetching a file from a URL.\\n\\n        It first checks if the file (ted_hrlr_translate_pt_en_converter.zip) already exists in the local cache. If not, it downloads it from the Google Cloud Storage URL provided.\\n\\n        The extract=True argument is very important here. After the download is complete, it automatically unzips the archive. T\\n        his creates a directory named ted_hrlr_translate_pt_en_converter which contains the actual model files.\\n\\n    Loading the Tokenizer Model (tf.saved_model.load): Once the model files are downloaded and extracted, this function loads them into memory.\\n\\n        It reads the specified directory and reconstructs the complete TensorFlow model, including its architecture, weights, \\n        and any associated assets like the vocabulary files needed for tokenization.\\n\\n        The resulting tokenizers object is a fully functional model. It acts as a container holding two separate sub-models: \\n        one for Portuguese (tokenizers.pt) and one for English (tokenizers.en). These can now be used directly to process the text data for training.\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' \n",
    "Step 3: Define the pre-trained Tokenizers for both languages\n",
    "1. https://www.youtube.com/watch?v=L5CR-k2ROu4 : Tokenization in NLP: Basics to Advanced\n",
    "2. https://www.tensorflow.org/text/guide/tokenizers \n",
    "3. https://www.tensorflow.org/text/guide/subwords_tokenizer\n",
    "\n",
    "Its main purpose is to download and load a pre-trained tokenizer model from a public repository hosted by TensorFlow.\n",
    "\n",
    "A tokenizer is a fundamental tool in Natural Language Processing (NLP). It handles two key tasks:\n",
    "\n",
    "    Tokenization: It breaks down raw text (like a sentence) into smaller units called \"tokens.\" These tokens are then mapped to unique numerical IDs. \n",
    "    Neural networks can only process numbers, so this step is essential to convert text into a format the model can understand.\n",
    "\n",
    "    Detokenization: It performs the reverse operation, converting the numerical output from the model back into human-readable text.\n",
    "\n",
    "The process in this snippet happens in two main steps:\n",
    "\n",
    "    Downloading and Extracting the Model (tf.keras.utils.get_file): This function is a convenient utility that handles fetching a file from a URL.\n",
    "\n",
    "        It first checks if the file (ted_hrlr_translate_pt_en_converter.zip) already exists in the local cache. If not, it downloads it from the Google Cloud Storage URL provided.\n",
    "\n",
    "        The extract=True argument is very important here. After the download is complete, it automatically unzips the archive. T\n",
    "        his creates a directory named ted_hrlr_translate_pt_en_converter which contains the actual model files.\n",
    "\n",
    "    Loading the Tokenizer Model (tf.saved_model.load): Once the model files are downloaded and extracted, this function loads them into memory.\n",
    "\n",
    "        It reads the specified directory and reconstructs the complete TensorFlow model, including its architecture, weights, \n",
    "        and any associated assets like the vocabulary files needed for tokenization.\n",
    "\n",
    "        The resulting tokenizers object is a fully functional model. It acts as a container holding two separate sub-models: \n",
    "        one for Portuguese (tokenizers.pt) and one for English (tokenizers.en). These can now be used directly to process the text data for training.\n",
    "\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea04d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the unique name of the pre-trained tokenizer model. This specific model\n",
    "# is designed for Portuguese-to-English translation and is provided by TensorFlow.\n",
    "model_name = 'ted_hrlr_translate_pt_en_converter'\n",
    "\n",
    "# Use a Keras utility function to download the tokenizer model from a public Google Cloud Storage URL.\n",
    "# This function is very convenient as it handles both downloading and extraction.\n",
    "tf.keras.utils.get_file(\n",
    "    # The local filename to save the downloaded archive as.\n",
    "    f'{model_name}.zip',\n",
    "    \n",
    "    # The public URL where the model archive is hosted.\n",
    "    f'https://storage.googleapis.com/download.tensorflow.org/models/{model_name}.zip',\n",
    "    \n",
    "    # Specifies the directory to cache the file. '.' means the current directory.\n",
    "    cache_dir='.',\n",
    "    \n",
    "    # Specifies not to use any sub-directory within the cache directory.\n",
    "    cache_subdir='',\n",
    "    \n",
    "    # This crucial argument tells the function to automatically extract the contents\n",
    "    # of the .zip file after it's downloaded. This creates a folder with the model files.\n",
    "    extract=True\n",
    ")\n",
    "# Load the complete TensorFlow SavedModel from the directory that was just extracted.\n",
    "# This function reconstructs the model, including its vocabulary and tokenization logic.\n",
    "# The resulting 'tokenizers' object will contain methods for both Portuguese and English.\n",
    "# With extract=True and cache_dir='.', the model should be extracted directly into\n",
    "# the directory named by model_name.\n",
    "tokenizers = tf.saved_model.load(f'./ted_hrlr_translate_pt_en_converter_extracted/ted_hrlr_translate_pt_en_converter')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43c6801e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Examples in Portuguese:\n",
      "e quando melhoramos a procura , tiramos a única vantagem da impressão , que é a serendipidade .\n",
      "mas e se estes fatores fossem ativos ?\n",
      "mas eles não tinham a curiosidade de me testar .\n",
      "\n",
      "> Examples in English:\n",
      "and when you improve searchability , you actually take away the one advantage of print , which is serendipity .\n",
      "but what if it were active ?\n",
      "but they did n't test for curiosity .\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-31 19:36:48.253450: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> This is a tokenized\n",
      "[2, 72, 117, 79, 1259, 1491, 2362, 13, 79, 150, 184, 311, 71, 103, 2308, 74, 2679, 13, 148, 80, 55, 4840, 1434, 2423, 540, 15, 3]\n",
      "[2, 87, 90, 107, 76, 129, 1852, 30, 3]\n",
      "[2, 87, 83, 149, 50, 9, 56, 664, 85, 2512, 15, 3]\n",
      "\n",
      "> This is the detokenized (human-readable) text:\n",
      "and when you improve searchability , you actually take away the one advantage of print , which is serendipity .\n",
      "but what if it were active ?\n",
      "but they did n ' t test for curiosity .\n"
     ]
    }
   ],
   "source": [
    "# Create a small batch of 3 sentence pairs from the training data to use as an example.\n",
    "# .take(1) ensures we only grab one batch.\n",
    "for pt_examples, en_examples in train_examples.batch(3).take(1):\n",
    "  \n",
    "  # --- 1. View the original Portuguese text ---\n",
    "  print('> Examples in Portuguese:')\n",
    "  # Loop through the Portuguese sentences in the batch.\n",
    "  # .numpy() converts the TensorFlow tensor to a NumPy array.\n",
    "  # .decode('utf-8') converts the raw bytes to a human-readable string.\n",
    "  for pt in pt_examples.numpy():\n",
    "    print(pt.decode('utf-8'))\n",
    "  print() # Add a blank line for readability.\n",
    "\n",
    "  # --- 2. View the original English text ---\n",
    "  print('> Examples in English:')\n",
    "  # Do the same for the English sentences.\n",
    "  for en in en_examples.numpy():\n",
    "    print(en.decode('utf-8'))\n",
    "  print()\n",
    "\n",
    "# --- 3. Tokenize the English text (Text -> Numbers) ---\n",
    "# Pass the batch of English sentences to the pre-trained English tokenizer.\n",
    "# This converts each sentence into a sequence of integer IDs.\n",
    "encoded = tokenizers.en.tokenize(en_examples)\n",
    "\n",
    "print('> This is a tokenized')\n",
    "# Loop through the resulting tensor of token IDs.\n",
    "# .to_list() converts the tensor to a standard Python list for easy printing.\n",
    "for row in encoded.to_list():\n",
    "  print(row)\n",
    "print()\n",
    "\n",
    "# --- 4. Detokenize the IDs back to text (Numbers -> Text) ---\n",
    "# This demonstrates the \"round trip\" to ensure the process is reversible.\n",
    "# We take the `encoded` tensor of IDs and convert it back to text.\n",
    "round_trip = tokenizers.en.detokenize(encoded)\n",
    "\n",
    "print('> This is the detokenized (human-readable) text:')\n",
    "# Loop through the resulting batch of detokenized text.\n",
    "for line in round_trip.numpy():\n",
    "  print(line.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3ac22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Step 4: Define the functions for preparing batches for training and evaluation\n",
    "\n",
    "What's Happening in the Code?\n",
    "\n",
    "This code defines two functions, prepare_batch and make_batches, that work together to create an efficient data pipeline using tf.data. \n",
    "This pipeline is designed to load, preprocess, shuffle, and batch the data, which is essential for training a deep learning model effectively.\n",
    "\n",
    "1. The prepare_batch function\n",
    "\n",
    "This function handles the preprocessing for a single batch of sentence pairs.\n",
    "\n",
    "    Tokenize and Truncate: It first converts the Portuguese and English text into numerical token IDs. It also enforces a maximum sequence length (MAX_TOKENS) by cutting off any sentences that are too long.\n",
    "\n",
    "    Create Decoder Input/Output: For the English (target) sentences, it creates two versions:\n",
    "\n",
    "        en_inputs: This is the input to the decoder. It includes every token except the last one.\n",
    "\n",
    "        en_labels: This is the target label that the model tries to predict. It includes every token except the first one.\n",
    "\n",
    "        This \"shift\" is a core concept in training sequence models, as it teaches the model to predict the next word in the sequence given the previous words.\n",
    "\n",
    "    Convert to Tensor: It converts the lists of token IDs into dense TensorFlow tensors, adding padding (with zeros) to make all sequences in the batch the same length.\n",
    "\n",
    "2. The make_batches function\n",
    "\n",
    "This function builds the complete data pipeline.\n",
    "\n",
    "    .shuffle(): Randomly shuffles the dataset. This is crucial for effective training, as it prevents the model from learning the order of the training examples. \n",
    "    The buffer_size tells the pipeline how many elements to load into a buffer to shuffle from.\n",
    "\n",
    "    .batch(): Groups the individual examples into batches of a specified size (e.g., 128).\n",
    "\n",
    "    .map(): Applies the prepare_batch function to each batch in parallel. tf.data.AUTOTUNE allows TensorFlow to dynamically adjust \n",
    "    the number of parallel threads to use, optimizing performance.\n",
    "\n",
    "    .prefetch(): This is a key optimization. It creates a background thread that prepares the next batch of data on the CPU while the \n",
    "    GPU is busy training on the current batch. This ensures the GPU doesn't have to wait, dramatically speeding up training.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ac261be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a maximum sequence length. Sentences with more tokens than this will be truncated.\n",
    "MAX_TOKENS = 128\n",
    "\n",
    "def prepare_batch(pt, en):\n",
    "    \"\"\"\n",
    "    This function tokenizes, truncates, and prepares the input and label tensors\n",
    "    for a single batch of Portuguese-English sentence pairs.\n",
    "    \"\"\"\n",
    "    # --- Process the Portuguese (Source) Sentences ---\n",
    "    # Convert the raw Portuguese text into sequences of token IDs.\n",
    "    pt = tokenizers.pt.tokenize(pt)\n",
    "    # Enforce the maximum sequence length by keeping only the first MAX_TOKENS.\n",
    "    pt = pt[:, :MAX_TOKENS]\n",
    "    # Convert the ragged (variable-length) tensor into a dense tensor,\n",
    "    # padding shorter sequences with zeros.\n",
    "    pt = pt.to_tensor()\n",
    "\n",
    "    # --- Process the English (Target) Sentences ---\n",
    "    # Convert the raw English text into sequences of token IDs.\n",
    "    # We allow one extra token for the start/end tokens that will be added implicitly.\n",
    "    en = tokenizers.en.tokenize(en)\n",
    "    en = en[:, :(MAX_TOKENS + 1)]\n",
    "\n",
    "    # Create the input to the decoder by taking all tokens except the last one.\n",
    "    en_inputs = en[:, :-1].to_tensor()\n",
    "\n",
    "    # Create the target labels for the model to predict by taking all tokens except the first one.\n",
    "    # This creates the \"teacher forcing\" mechanism where the model learns to predict the next token.\n",
    "    en_labels = en[:, 1:].to_tensor()\n",
    "\n",
    "    # Return a tuple where the first element is the model's input (a pair of tensors)\n",
    "    # and the second element is the target label.\n",
    "    return (pt, en_inputs), en_labels\n",
    "\n",
    "def make_batches(ds, buffer_size=20000, batch_size=512):\n",
    "    \"\"\"\n",
    "    Builds an efficient, optimized tf.data pipeline from the dataset.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        ds\n",
    "        # Shuffle the dataset to ensure the model doesn't learn the order of examples.\n",
    "        # A large buffer size improves the randomness of the shuffle.\n",
    "        .shuffle(buffer_size)\n",
    "        # Group the individual examples into batches of the specified size.\n",
    "        .batch(batch_size)\n",
    "        # Apply the `prepare_batch` function to each batch in parallel for efficiency.\n",
    "        # tf.data.AUTOTUNE lets TensorFlow figure out the best level of parallelism.\n",
    "        .map(prepare_batch, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        # Pre-fetch the next batch of data while the current one is being processed on the GPU.\n",
    "        # This is a crucial performance optimization that prevents data bottlenecks.\n",
    "        .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    )\n",
    "\n",
    "# --- Create the final data pipelines for training and validation ---\n",
    "# The same pipeline logic is applied to both the training and validation datasets.\n",
    "train_batches = make_batches(train_examples)\n",
    "val_batches = make_batches(val_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8cd262",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Step 5: Define Positional Encoding to add the position information to the tokens \n",
    "https://arxiv.org/pdf/1706.03762 :  Original Paper\n",
    "\n",
    "This code defines a custom Keras layer, PositionalEmbedding, that performs two critical jobs:\n",
    "\n",
    "    Word Embedding: It converts the numerical token IDs (like [8, 64, 12, 5]) into dense vector representations (embeddings). \n",
    "    Each unique word in the vocabulary gets its own unique vector. This is done by the standard tf.keras.layers.Embedding layer.\n",
    "\n",
    "    Positional Encoding: It creates a second vector that represents the position of each word in the sentence (e.g., 1st word, 2nd word, etc.). \n",
    "    This positional vector is then added to the word embedding vector.\n",
    "\n",
    "1. The positional_encoding function\n",
    "\n",
    "This function implements the clever mathematical trick from the original \"Attention Is All You Need\" paper.\n",
    "\n",
    "    It generates a unique positional vector for each position in a sequence.\n",
    "\n",
    "    It uses a combination of sine and cosine functions at different frequencies.\n",
    "\n",
    "    The key idea: This method allows the model to easily learn relative positions. \n",
    "    Because of the properties of sine and cosine, the positional encoding for position + k can be represented as a linear function of the encoding for position. \n",
    "    This makes it easy for the model to understand how far apart words are, which is crucial for understanding context and grammar.\n",
    "\n",
    "    \n",
    "2. The PositionalEmbedding Layer\n",
    "\n",
    "This Keras layer brings everything together.\n",
    "\n",
    "    When it receives a batch of tokenized sentences, it first looks up the word embedding for each token.\n",
    "\n",
    "    It then scales these embeddings (a standard practice from the paper).\n",
    "\n",
    "    Finally, it adds the pre-calculated positional encoding vector corresponding to each word's position.\n",
    "\n",
    "    The final output is a single vector for each word that contains information about both what the word is and where it is in the sentence.   \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2386fc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(length, depth):\n",
    "    \"\"\"\n",
    "    Generates a matrix of positional encodings. This is a clever way to inject\n",
    "    information about the order of tokens in the sequence.\n",
    "\n",
    "    Args:\n",
    "      length: The maximum length of the sequence.\n",
    "      depth: The dimensionality of the embedding (d_model).\n",
    "    \"\"\"\n",
    "    # The depth is split in half for the sine and cosine parts.\n",
    "    depth = depth/2\n",
    "\n",
    "    # Create a column vector of positions from 0 to length-1.\n",
    "    # Shape: (length, 1)\n",
    "    positions = np.arange(length)[:, np.newaxis]\n",
    "\n",
    "    # Create a row vector of depths.\n",
    "    # Shape: (1, depth)\n",
    "    depths = np.arange(depth)[np.newaxis, :]/depth\n",
    "\n",
    "    # Calculate the angle rates using the formula from the paper.\n",
    "    # The frequencies of the sine/cosine waves decrease along the depth dimension.\n",
    "    angle_rates = 1 / (10000**depths)\n",
    "\n",
    "    # Calculate the angle radians for each position and depth.\n",
    "    # This is the core of the positional encoding calculation.\n",
    "    angle_rads = positions * angle_rates\n",
    "\n",
    "    # Create the final positional encoding matrix by concatenating the sine and cosine values.\n",
    "    # The two are interleaved to create a complete encoding for each position.\n",
    "    pos_encoding = np.concatenate(\n",
    "        [np.sin(angle_rads), np.cos(angle_rads)],\n",
    "        axis=-1)\n",
    "\n",
    "    # Convert the NumPy array to a TensorFlow tensor.\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "\n",
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    This layer combines a standard word embedding with the positional encoding.\n",
    "    The output is a single tensor that contains both semantic and positional information.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # Standard embedding layer that maps token IDs to vectors.\n",
    "        # `mask_zero=True` tells the layer to ignore padding (zeros) in the input.\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True)\n",
    "\n",
    "        # Pre-calculate the positional encoding matrix.\n",
    "        # We create it for a long sequence (2048) so it can handle any sentence length during training.\n",
    "        self.pos_encoding = positional_encoding(length=2048, depth=d_model)\n",
    "\n",
    "    def compute_mask(self, *args, **kwargs):\n",
    "        # This ensures that the padding mask is correctly propagated through the network.\n",
    "        # The attention layers will use this mask to ignore padding tokens.\n",
    "        return self.embedding.compute_mask(*args, **kwargs)\n",
    "\n",
    "    def call(self, x):\n",
    "        # Get the length of the input sequence.\n",
    "        length = tf.shape(x)[1]\n",
    "\n",
    "        # 1. Get the word embeddings for the input tokens.\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # 2. Scale the embeddings. This is a standard practice from the paper that helps\n",
    "        # moderate the magnitude of the embedding vectors.\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "\n",
    "        # 3. Add the positional encodings to the word embeddings.\n",
    "        # This is where the position information is injected.\n",
    "        # We slice the pre-calculated pos_encoding to match the length of the input sequence.\n",
    "        x = x + self.pos_encoding[tf.newaxis, :length, :]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b7a05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Step 6:  Defines three custom Keras layers that implement the different types of attention used in the Transformer architecture. \n",
    "It uses a clever object-oriented approach by creating a BaseAttention class to hold the common components, and \n",
    "then two specialized classes, CrossAttention and GlobalSelfAttention, inherit from it.\n",
    "\n",
    "1. The BaseAttention Layer\n",
    "\n",
    "This is a parent class that isn't used directly but serves as a blueprint. It contains the three components that every attention block in a Transformer needs:\n",
    "\n",
    "    Multi-Head Attention (mha): This is the core engine. Instead of just calculating attention once, it does it multiple times in parallel (in different \"heads\"). \n",
    "    Each head can focus on a different aspect of the sentence's meaning or structure. This allows the model to capture a much richer understanding of the \n",
    "    relationships between words.\n",
    "\n",
    "    Layer Normalization (layernorm): A technique that stabilizes the training of deep neural networks. It normalizes the outputs of the attention layer, \n",
    "    which helps prevent the values from becoming too large or too small and improves the flow of gradients during backpropagation.\n",
    "\n",
    "    Add (add): This implements the \"residual connection\" (or \"skip connection\"). The input to the attention layer is added directly to its output. \n",
    "    This is a critical technique that allows the model to train much deeper networks by ensuring that information from earlier layers can easily \n",
    "    pass through to later layers without being lost.\n",
    "\n",
    "2. The GlobalSelfAttention Layer (or commonly called just self-attention)\n",
    "\n",
    "This layer is used inside both the Encoder and the Decoder. Its job is to process a single sequence and allow every word in that sequence to \"attend\" to every other word.\n",
    "\n",
    "    How it works: The query, key, and value inputs to the multi-head attention layer all come from the same source (x).\n",
    "\n",
    "    Purpose: It helps the model build a rich, context-aware representation of each word. For example, in the sentence \n",
    "    \"The tired animal crossed the road,\" this layer helps the model understand that \"The\" and \"tired\" are related to \"animal.\"\n",
    "\n",
    "3. The CrossAttention Layer\n",
    "\n",
    "This layer is the bridge between the Encoder and the Decoder and is only used in the Decoder. \n",
    "It's how the model looks at the source sentence while generating the translated sentence.\n",
    "\n",
    "    How it works:\n",
    "\n",
    "        The query comes from the Decoder's own sequence (the translated words generated so far).\n",
    "\n",
    "        The key and value come from the output of the Encoder (the context from the source sentence).\n",
    "\n",
    "    Purpose: It allows the Decoder to decide which words in the source sentence are most important for predicting the next word in the translation. \n",
    "    For example, when translating the Portuguese word \"estava\" to English, this layer helps the model look at the context in the Portuguese \n",
    "    sentence to decide whether the correct translation is \"was,\" \"were,\" or \"is.\"\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8cb1877c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    A base class for attention layers that contains the common components:\n",
    "    Multi-Head Attention, Layer Normalization, and a Residual Add connection.\n",
    "    This promotes code reuse and follows the standard Transformer block structure.\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        # The core Multi-Head Attention layer. The **kwargs allows us to pass\n",
    "        # configuration parameters like num_heads, key_dim, etc., directly to it.\n",
    "        self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
    "        \n",
    "        # Layer Normalization helps stabilize the network and speeds up training.\n",
    "        self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "        \n",
    "        # The Add layer is used for the residual connection.\n",
    "        self.add = tf.keras.layers.Add()\n",
    "\n",
    "class CrossAttention(BaseAttention):\n",
    "    \"\"\"\n",
    "    Implements the cross-attention mechanism. This layer is used in the Decoder\n",
    "    to attend to the output of the Encoder. It connects the two parts of the model.\n",
    "    \"\"\"\n",
    "    def call(self, x, context):\n",
    "        # The mha layer calculates the attention scores and output.\n",
    "        # `query` comes from the decoder's sequence (x).\n",
    "        # `key` and `value` come from the encoder's output (context).\n",
    "        attn_output, attn_scores = self.mha(\n",
    "            query=x,\n",
    "            key=context,\n",
    "            value=context,\n",
    "            return_attention_scores=True)  # We request scores for visualization.\n",
    "\n",
    "        # Cache the attention scores for later visualization and analysis.\n",
    "        self.last_attn_scores = attn_scores\n",
    "\n",
    "        # Apply the residual connection: add the input (x) to the attention output.\n",
    "        # This helps with gradient flow in deep networks.\n",
    "        x = self.add([x, attn_output])\n",
    "        \n",
    "        # Apply layer normalization to the result of the residual connection.\n",
    "        x = self.layernorm(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class GlobalSelfAttention(BaseAttention):\n",
    "    \"\"\"\n",
    "    Implements the global self-attention mechanism. This layer is used within\n",
    "    the Encoder and Decoder to process a single sequence.\n",
    "    \"\"\"\n",
    "    def call(self, x):\n",
    "        # The mha layer calculates the attention scores and output.\n",
    "        # For self-attention, the query, key, and value are all the same tensor (x).\n",
    "        # This allows every token in the sequence to attend to every other token.\n",
    "        attn_output = self.mha(\n",
    "            query=x,\n",
    "            value=x,\n",
    "            key=x)\n",
    "            \n",
    "        # Apply the residual connection: add the input (x) to the attention output.\n",
    "        x = self.add([x, attn_output])\n",
    "        \n",
    "        # Apply layer normalization.\n",
    "        x = self.layernorm(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b8b19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Step 7: defines a custom Keras layer that implements the \"Position-wise Feed-Forward Network\" described in the original \"Attention Is All You Need\" paper. \n",
    "It's a relatively simple component, but it's essential for the model's performance.\n",
    "\n",
    "The structure of this layer is identical to the attention sub-layer that comes before it: it consists of the main processing block followed by a \n",
    "residual connection and layer normalization.\n",
    "\n",
    "The Core Network (self.seq)\n",
    "\n",
    "The main processing is done by a tf.keras.Sequential model containing three layers:\n",
    "\n",
    "    An \"Expansion\" Dense Layer: The first Dense layer takes the output from the attention sub-layer and expands its dimensionality from d_model (e.g., 256) \n",
    "    to a much larger intermediate dimension, dff (e.g., 1024). It uses a ReLU activation function, which introduces non-linearity into the model. \n",
    "    This expansion allows the model to learn more complex relationships and features.\n",
    "\n",
    "    A \"Contraction\" Dense Layer: The second Dense layer projects the data from the large dff dimension back down to the original d_model dimension.\n",
    "\n",
    "    Dropout: A Dropout layer is applied for regularization. During training, it randomly sets a fraction of its input units to zero at each update step,\n",
    "      which helps prevent overfitting by making the network less reliant on any single neuron.\n",
    "\n",
    "Residual Connection and Layer Normalization\n",
    "\n",
    "Just like in the attention sub-layer, this FeedForward network is wrapped with two crucial components:\n",
    "\n",
    "    self.add: Implements the residual (or \"skip\") connection. The input that came into the FeedForward layer is added directly to its output. \n",
    "    This helps gradients flow through the deep network and prevents information from being lost.\n",
    "\n",
    "    self.layer_norm: Applies layer normalization to the result of the residual connection, which stabilizes training.\n",
    "\n",
    "In simple terms: The FeedForward network's role is to take the context-rich vectors produced by the attention mechanism and perform additional \n",
    "non-linear transformations on each one independently. This deepens the model and gives it a greater capacity to learn the complex patterns required for translation.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2857f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Implements the Position-wise Feed-Forward Network (FFN) from the Transformer paper.\n",
    "    This layer is applied to each position separately and identically.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, dff, dropout_rate=0.1):\n",
    "        \"\"\"\n",
    "        Initializes the FeedForward layer.\n",
    "\n",
    "        Args:\n",
    "          d_model: The dimensionality of the model's embeddings (e.g., 256).\n",
    "          dff: The dimensionality of the inner-layer of the FFN (e.g., 1024).\n",
    "          dropout_rate: The fraction of input units to drop for regularization.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # The core of the FFN is a two-layer fully-connected network.\n",
    "        self.seq = tf.keras.Sequential([\n",
    "            # Layer 1: Expands the input from d_model to dff and applies ReLU activation.\n",
    "            tf.keras.layers.Dense(dff, activation='relu'),\n",
    "            \n",
    "            # Layer 2: Projects the output from dff back down to d_model.\n",
    "            tf.keras.layers.Dense(d_model),\n",
    "            \n",
    "            # A dropout layer for regularization to prevent overfitting.\n",
    "            tf.keras.layers.Dropout(dropout_rate)\n",
    "        ])\n",
    "        \n",
    "        # An Add layer for the residual connection.\n",
    "        self.add = tf.keras.layers.Add()\n",
    "        \n",
    "        # A Layer Normalization layer for stabilizing the training.\n",
    "        self.layer_norm = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "    def call(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass for the layer.\n",
    "        \"\"\"\n",
    "        # Pass the input through the sequential feed-forward network.\n",
    "        seq_out = self.seq(x)\n",
    "        \n",
    "        # Apply the residual connection: add the original input (x) to the FFN's output.\n",
    "        x = self.add([x, seq_out])\n",
    "        \n",
    "        # Apply layer normalization to the result of the residual connection.\n",
    "        x = self.layer_norm(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007485ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Step 8: defines two classes: EncoderLayer and Encoder.\n",
    "\n",
    "1. EncoderLayer: A Single Processing Block\n",
    "\n",
    "Think of an EncoderLayer as one step in an assembly line. It takes a sequence of word vectors as input and performs two main operations \n",
    "on them before passing them to the next step.\n",
    "\n",
    "    self_attention (GlobalSelfAttention): First, the data goes through a self-attention mechanism. This allows every word in the sentence to look at \n",
    "    every other word to gather context. For example, in the sentence \"The green car is fast,\" this layer helps the model understand that \"green\" is describing the \"car.\"\n",
    "\n",
    "    ffn (FeedForward): Second, the output from the attention layer is passed through a position-wise feed-forward network. This network processes each word's \n",
    "    vector individually, performing further non-linear transformations to help the model learn more complex features.\n",
    "\n",
    "Each EncoderLayer is a self-contained processing unit that refines the representation of the input sentence.\n",
    "\n",
    "Encoder: The Full Assembly Line\n",
    "\n",
    "The Encoder is the complete assembly line, made up of a stack of multiple EncoderLayers.\n",
    "\n",
    "    pos_embedding (PositionalEmbedding): The process starts here. The raw input (a sequence of token IDs) is passed to the PositionalEmbedding layer. \n",
    "    This layer converts the token IDs into vectors that contain information about both what the word is (its meaning) and where it is in the sentence (its position).\n",
    "\n",
    "    dropout: A dropout layer is applied to the embeddings for regularization, which helps prevent the model from overfitting.\n",
    "\n",
    "    The Stack of enc_layers: The core of the Encoder is a loop that passes the data through each EncoderLayer in the stack, one after the other. \n",
    "    If you have num_layers=4, the data will be processed by four of these blocks sequentially. Each layer further refines the context and meaning of the words in the sentence.\n",
    "\n",
    "The final output of the Encoder is a set of context-rich vectors (one for each word in the input sentence) that is then passed to the Decoder.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "68e9a85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    A single layer of the Transformer Encoder. It consists of two sub-layers:\n",
    "    1. A Global Self-Attention mechanism.\n",
    "    2. A Position-wise Feed-Forward Network.\n",
    "    Each sub-layer has a residual connection followed by layer normalization.\n",
    "    \"\"\"\n",
    "    def __init__(self,*, d_model, num_heads, dff, dropout_rate=0.1):\n",
    "        \"\"\"\n",
    "        Initializes the EncoderLayer.\n",
    "\n",
    "        Args:\n",
    "          d_model: The dimensionality of the model's embeddings.\n",
    "          num_heads: The number of attention heads.\n",
    "          dff: The dimensionality of the inner-layer of the FFN.\n",
    "          dropout_rate: The dropout rate for regularization.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # The first sub-layer: a global self-attention mechanism.\n",
    "        self.self_attention = GlobalSelfAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=d_model,\n",
    "            dropout=dropout_rate)\n",
    "\n",
    "        # The second sub-layer: a position-wise feed-forward network.\n",
    "        self.ffn = FeedForward(d_model, dff)\n",
    "\n",
    "    def call(self, x):\n",
    "        \"\"\"Defines the forward pass for the layer.\"\"\"\n",
    "        # Pass the input through the self-attention layer.\n",
    "        x = self.self_attention(x)\n",
    "        \n",
    "        # Pass the result through the feed-forward network.\n",
    "        x = self.ffn(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    The complete Transformer Encoder, which is a stack of N identical EncoderLayers.\n",
    "    \"\"\"\n",
    "    def __init__(self, *, num_layers, d_model, num_heads,\n",
    "                 dff, vocab_size, dropout_rate=0.1):\n",
    "        \"\"\"\n",
    "        Initializes the Encoder.\n",
    "\n",
    "        Args:\n",
    "          num_layers: The number of EncoderLayers to stack.\n",
    "          d_model: The dimensionality of the model's embeddings.\n",
    "          num_heads: The number of attention heads.\n",
    "          dff: The dimensionality of the inner-layer of the FFN.\n",
    "          vocab_size: The size of the input vocabulary.\n",
    "          dropout_rate: The dropout rate for regularization.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # The first layer is the positional embedding layer, which adds word\n",
    "        # meaning and position information to the input tokens.\n",
    "        self.pos_embedding = PositionalEmbedding(\n",
    "            vocab_size=vocab_size, d_model=d_model)\n",
    "\n",
    "        # Create a list containing `num_layers` instances of EncoderLayer.\n",
    "        # This forms the main stack of the encoder.\n",
    "        self.enc_layers = [\n",
    "            EncoderLayer(d_model=d_model,\n",
    "                         num_heads=num_heads,\n",
    "                         dff=dff,\n",
    "                         dropout_rate=dropout_rate)\n",
    "            for _ in range(num_layers)]\n",
    "        \n",
    "        # A dropout layer for regularization, applied after the embedding.\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, x):\n",
    "        \"\"\"Defines the forward pass for the entire Encoder.\"\"\"\n",
    "        # 1. Get the embeddings with positional information.\n",
    "        x = self.pos_embedding(x)\n",
    "\n",
    "        # 2. Apply dropout to the embeddings.\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # 3. Pass the embeddings through the stack of N encoder layers.\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x)\n",
    "\n",
    "        # The final output is a sequence of context-rich vectors.\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b914abcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Step 9: defines three classes that build the Decoder: CausalSelfAttention, DecoderLayer, and Decoder.\n",
    "\n",
    "\n",
    "\"Decoder\" side of the Transformer: Its job is to take the encoded representation of the source sentence (from the Encoder) and generate the translated sentence word by word.\n",
    "\n",
    "1. CausalSelfAttention: The \"No Cheating\" Attention\n",
    "\n",
    "This is a special type of self-attention used only in the Decoder. Its purpose is to prevent the model from \"cheating\" during training.\n",
    "\n",
    "    The Problem: When training the model to predict the next word in a sentence, we must ensure it only uses the words that came before it. \n",
    "    For example, to predict the 4th word, it should only be allowed to see words 1, 2, and 3. If it could see word 4 or 5, the task would be trivial and the model wouldn't learn anything.\n",
    "\n",
    "    The Solution: CausalSelfAttention implements this rule. The key is the use_causal_mask=True argument. This automatically creates a \"look-ahead mask\" that hides all \n",
    "    future tokens in the sequence. For any given word, this mask blocks its attention mechanism from seeing any words that appear later in the sentence. \n",
    "    This is why it's called \"causal\"—it enforces the cause-and-effect flow of time in a sentence.\n",
    "\n",
    "2. DecoderLayer: The Decoder's Processing Block\n",
    "\n",
    "This is a single layer of the Decoder. It's more complex than an EncoderLayer because it has three sub-layers instead of two.\n",
    "\n",
    "    Masked Self-Attention (CausalSelfAttention): First, the Decoder processes its own input sequence (the translation generated so far) using causal self-attention. \n",
    "    This allows it to gather context from the words it has already predicted, without peeking at the future.\n",
    "\n",
    "    Cross-Attention: This is the most important step. The output from the self-attention layer is then passed to a cross-attention layer. \n",
    "    This layer takes the context from the Encoder (the representation of the source sentence) and allows the Decoder to decide which words \n",
    "    in the source sentence are most relevant for predicting the next word in the translation.\n",
    "\n",
    "    Feed-Forward Network (ffn): Finally, the output from the cross-attention layer is passed through a standard \n",
    "    feed-forward network for further processing, just like in the Encoder.\n",
    "\n",
    "3. Decoder: The Full Translation Engine\n",
    "\n",
    "The Decoder is the complete stack of DecoderLayers. It orchestrates the entire translation process.\n",
    "\n",
    "    Input: It takes two inputs: the target sequence x (the words translated so far) and the context (the output from the Encoder).\n",
    "\n",
    "    Embedding and Dropout: It starts by applying positional embeddings and dropout to the target sequence x.\n",
    "\n",
    "    The Stack of dec_layers: It then passes the data through the stack of DecoderLayers. \n",
    "    In each layer, the three-step process (masked self-attention, cross-attention, feed-forward) is repeated, progressively refining the translation.\n",
    "\n",
    "    Output: The final output of the Decoder is a sequence of vectors that is then passed to a final linear layer to be \n",
    "    converted into probability scores for the next word in the vocabulary.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39f4f084",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(BaseAttention):\n",
    "  \"\"\"\n",
    "  Implements causal (or \"look-ahead masked\") self-attention. This is crucial\n",
    "  for the Decoder to prevent it from \"cheating\" by looking at future tokens\n",
    "  in the sequence it's trying to predict.\n",
    "  \"\"\"\n",
    "  def call(self, x):\n",
    "    # The MultiHeadAttention layer is called with `use_causal_mask=True`.\n",
    "    # This automatically creates a mask that ensures for any position `i`,\n",
    "    # the attention mechanism can only see tokens at positions `j <= i`.\n",
    "    attn_output = self.mha(\n",
    "        query=x,\n",
    "        value=x,\n",
    "        key=x,\n",
    "        use_causal_mask = True)\n",
    "    \n",
    "    # Apply the residual connection and layer normalization.\n",
    "    x = self.add([x, attn_output])\n",
    "    x = self.layernorm(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    A single layer of the Transformer Decoder. It consists of three sub-layers:\n",
    "    1. A Causal Self-Attention mechanism (for the target sequence).\n",
    "    2. A Cross-Attention mechanism (to attend to the encoder's output).\n",
    "    3. A Position-wise Feed-Forward Network.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 *,\n",
    "                 d_model,\n",
    "                 num_heads,\n",
    "                 dff,\n",
    "                 dropout_rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        # Sub-layer 1: Causal self-attention for the target sequence.\n",
    "        # This is the corrected version.\n",
    "        self.causal_self_attention = CausalSelfAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=d_model,\n",
    "            dropout=dropout_rate)\n",
    "\n",
    "        # Sub-layer 2: Cross-attention to look at the encoder's output (context).\n",
    "        self.cross_attention = CrossAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=d_model,\n",
    "            dropout=dropout_rate)\n",
    "\n",
    "        # Sub-layer 3: The position-wise feed-forward network.\n",
    "        self.ffn = FeedForward(d_model, dff)\n",
    "\n",
    "    def call(self, x, context):\n",
    "        \"\"\"Defines the forward pass for the layer.\"\"\"\n",
    "        # Pass the input through the causal self-attention layer.\n",
    "        x = self.causal_self_attention(x=x)\n",
    "        \n",
    "        # Pass the result through the cross-attention layer, using the\n",
    "        # encoder's output as the context.\n",
    "        x = self.cross_attention(x=x, context=context)\n",
    "\n",
    "        # Cache the attention scores from the cross-attention layer for visualization.\n",
    "        self.last_attn_scores = self.cross_attention.last_attn_scores\n",
    "\n",
    "        # Pass the result through the feed-forward network.\n",
    "        x = self.ffn(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    The complete Transformer Decoder, which is a stack of N identical DecoderLayers.\n",
    "    \"\"\"\n",
    "    def __init__(self, *, num_layers, d_model, num_heads, dff, vocab_size,\n",
    "                 dropout_rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # The positional embedding layer for the target sequence.\n",
    "        self.pos_embedding = PositionalEmbedding(vocab_size=vocab_size,\n",
    "                                                 d_model=d_model)\n",
    "        \n",
    "        # A dropout layer for regularization.\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "        # Create a list containing `num_layers` instances of DecoderLayer.\n",
    "        self.dec_layers = [\n",
    "            DecoderLayer(d_model=d_model, num_heads=num_heads,\n",
    "                         dff=dff, dropout_rate=dropout_rate)\n",
    "            for _ in range(num_layers)]\n",
    "        \n",
    "        # A placeholder to store the attention scores from the last layer.\n",
    "        self.last_attn_scores = None\n",
    "\n",
    "    def call(self, x, context):\n",
    "        \"\"\"Defines the forward pass for the entire Decoder.\"\"\"\n",
    "        # 1. Get the embeddings with positional information for the target sequence.\n",
    "        x = self.pos_embedding(x)\n",
    "\n",
    "        # 2. Apply dropout to the embeddings.\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # 3. Pass the data through the stack of N decoder layers.\n",
    "        # Both the target sequence (x) and the encoder's output (context) are passed.\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.dec_layers[i](x, context)\n",
    "\n",
    "        # 4. Cache the attention scores from the final decoder layer.\n",
    "        self.last_attn_scores = self.dec_layers[-1].last_attn_scores\n",
    "\n",
    "        # The final output is a sequence of vectors ready for the final classification layer.\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e950a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Step 10: assembles the Encoder and Decoder into the complete Transformer model. It defines the exact flow of data from the source sentence \n",
    "all the way to the final prediction for the translated sentence.\n",
    "\n",
    "The Transformer class inherits from tf.keras.Model, which is the standard way to create custom, complex models in TensorFlow. It brings together all the pieces we've discussed previously.\n",
    "\n",
    "The __init__ Method (Building the Model)\n",
    "\n",
    "The __init__ method is the constructor. Its job is to build and initialize the three main parts of the model:\n",
    "\n",
    "    The Encoder (self.encoder): An instance of the Encoder class we defined earlier. It is configured with the hyperparameters for the input language \n",
    "    (e.g., Portuguese vocabulary size).\n",
    "\n",
    "    The Decoder (self.decoder): An instance of the Decoder class. It is configured with the hyperparameters for the target language (e.g., English vocabulary size).\n",
    "\n",
    "    The Final Layer (self.final_layer): This is a standard Dense (fully-connected) layer. Its job is to take the final processed vectors from the Decoder and convert \n",
    "    them into a score for every single word in the target vocabulary. The word with the highest score is the model's prediction for the next token in the sequence. \n",
    "    This layer is often called the \"output projection\" or \"classification\" layer.\n",
    "\n",
    "The call Method (The Forward Pass)\n",
    "\n",
    "The call method defines how data flows through the model during training and inference. It's a clear, three-step process:\n",
    "\n",
    "    Encode the Input: The source sentence (context) is passed into the self.encoder. The encoder processes it and produces a set of context-rich vectors. \n",
    "    This encoded context captures the meaning of the entire source sentence.\n",
    "\n",
    "    Decode with Context: The target sentence (x) and the context from the encoder are both passed into the self.decoder. The decoder uses its self-attention to process x \n",
    "    and its cross-attention to look at the context, figuring out how to generate the translation.\n",
    "\n",
    "    Generate Final Predictions: The output from the decoder is passed to the self.final_layer to produce the final output, called logits. These are the raw, \n",
    "    unnormalized prediction scores for each word in the target vocabulary. These logits are then used by the loss function to calculate how \"wrong\" the model was and to update its weights.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1f1037d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    The complete Transformer model, which encapsulates the Encoder, Decoder,\n",
    "    and the final linear layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, *, num_layers, d_model, num_heads, dff,\n",
    "                 input_vocab_size, target_vocab_size, dropout_rate=0.1):\n",
    "        \"\"\"\n",
    "        Initializes the Transformer model.\n",
    "\n",
    "        Args:\n",
    "          num_layers: The number of layers for both the encoder and decoder.\n",
    "          d_model: The dimensionality of the model's embeddings.\n",
    "          num_heads: The number of attention heads.\n",
    "          dff: The dimensionality of the inner-layer of the FFN.\n",
    "          input_vocab_size: The size of the source language's vocabulary.\n",
    "          target_vocab_size: The size of the target language's vocabulary.\n",
    "          dropout_rate: The dropout rate for regularization.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Instantiate the Encoder component.\n",
    "        self.encoder = Encoder(num_layers=num_layers, d_model=d_model,\n",
    "                               num_heads=num_heads, dff=dff,\n",
    "                               vocab_size=input_vocab_size,\n",
    "                               dropout_rate=dropout_rate)\n",
    "\n",
    "        # Instantiate the Decoder component.\n",
    "        self.decoder = Decoder(num_layers=num_layers, d_model=d_model,\n",
    "                               num_heads=num_heads, dff=dff,\n",
    "                               vocab_size=target_vocab_size,\n",
    "                               dropout_rate=dropout_rate)\n",
    "\n",
    "        # The final linear layer that maps the decoder's output to the target\n",
    "        # vocabulary space, producing the final prediction scores (logits).\n",
    "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        Defines the forward pass for the entire model.\n",
    "\n",
    "        Args:\n",
    "          inputs: A tuple containing the source sequence (context) and the\n",
    "                  target sequence (x).\n",
    "        \"\"\"\n",
    "        # Unpack the inputs. `context` is the source sentence (e.g., Portuguese),\n",
    "        # and `x` is the target sentence (e.g., English).\n",
    "        context, x  = inputs\n",
    "\n",
    "        # 1. Pass the source sentence through the encoder to get its contextual representation.\n",
    "        context = self.encoder(context)\n",
    "\n",
    "        # 2. Pass the target sentence and the encoder's output (context) through the decoder.\n",
    "        x = self.decoder(x, context)\n",
    "\n",
    "        # 3. Pass the decoder's output through the final linear layer to get the logits.\n",
    "        logits = self.final_layer(x)\n",
    "\n",
    "        # This try-except block is a non-standard way to handle potential masking issues.\n",
    "        # It's generally better to handle this by ensuring the final layer's dtype is\n",
    "        # float32, especially when using mixed precision. This block can often be removed.\n",
    "        try:\n",
    "            del logits._keras_mask\n",
    "        except AttributeError:\n",
    "            pass\n",
    "\n",
    "        # Return the final prediction scores (logits).\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6e6a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Masked loss and accuracy\n",
    "\n",
    "The Problem: Padding Tokens\n",
    "\n",
    "In a batch of sentences, each sentence can have a different length. To process them efficiently on a GPU, we need to make them all the same length. \n",
    "We do this by adding special padding tokens (represented by the ID 0) to the end of the shorter sentences.\n",
    "\n",
    "However, we don't want the model to learn to predict these padding tokens. The model's performance should only be measured on the actual words in the sequence. \n",
    "This is where \"masking\" comes in.\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f6dea891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function and metrics\n",
    "def masked_loss(label, pred):\n",
    "    \"\"\"\n",
    "    Calculates the cross-entropy loss, but intelligently ignores padded tokens.\n",
    "    \"\"\"\n",
    "    # 1. Create a boolean mask. It's `True` for any token that is NOT a\n",
    "    #    padding token (where the label is not 0) and `False` otherwise.\n",
    "    mask = label != 0\n",
    "\n",
    "    # 2. Create the standard loss object. `from_logits=True` is important because\n",
    "    #    our model outputs raw scores (logits), not probabilities.\n",
    "    #    `reduction='none'` is crucial: it returns the loss for each token individually.\n",
    "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True, reduction='none')\n",
    "\n",
    "    # 3. Calculate the loss for every single token in the batch.\n",
    "    loss = loss_object(label, pred)\n",
    "\n",
    "    # 4. Apply the mask to the loss. The mask is cast to the same dtype as the loss\n",
    "    #    (so True->1.0, False->0.0) and multiplied. This effectively sets the loss\n",
    "    #    for all padding tokens to zero.\n",
    "    mask = tf.cast(mask, dtype=loss.dtype)\n",
    "    loss *= mask\n",
    "\n",
    "    # 5. Calculate the final average loss. We sum up the total loss (where padding\n",
    "    #    loss is zero) and divide by the number of real (non-padded) tokens.\n",
    "    loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
    "    return loss\n",
    "\n",
    "def masked_accuracy(label, pred):\n",
    "    \"\"\"\n",
    "    Calculates the prediction accuracy, but intelligently ignores padded tokens.\n",
    "    \"\"\"\n",
    "    # 1. Get the model's prediction by finding the token with the highest score (logit).\n",
    "    pred = tf.argmax(pred, axis=2)\n",
    "    \n",
    "    # Ensure the label and prediction have the same data type for comparison.\n",
    "    label = tf.cast(label, pred.dtype)\n",
    "    \n",
    "    # 2. Check where the prediction matches the true label.\n",
    "    match = label == pred\n",
    "    \n",
    "    # 3. Create the same padding mask as in the loss function.\n",
    "    mask = label != 0\n",
    "    \n",
    "    # 4. Apply the mask to the matches. A position is now considered a \"true match\"\n",
    "    #    only if the prediction was correct AND it was not a padding token.\n",
    "    match = match & mask\n",
    "    \n",
    "    # Cast the boolean tensors to floats for calculation (True->1.0, False->0.0).\n",
    "    match = tf.cast(match, dtype=tf.float32)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    \n",
    "    # 5. Calculate the final accuracy: the total number of correct, non-padded\n",
    "    #    predictions divided by the total number of non-padded tokens.\n",
    "    return tf.reduce_sum(match)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdc11f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Step 11: Define loss function, optimizer and training loop\n",
    "\n",
    "\n",
    "a set of hyperparameters is defined. These are the key settings that control the size and shape of the model:\n",
    "\n",
    "    num_layers: The number of encoder and decoder layers to stack. A deeper model (more layers) can learn more complex patterns but is slower to train.\n",
    "\n",
    "    d_model: The main dimensionality of the embeddings throughout the model. This is a crucial parameter that affects the model's capacity.\n",
    "\n",
    "    dff: The \"inner\" dimension of the feed-forward networks. The expansion to this larger size allows the model to learn more complex features.\n",
    "\n",
    "    num_heads: The number of parallel attention \"heads.\" More heads allow the model to focus on different parts of the sentence structure simultaneously.\n",
    "\n",
    "    dropout_rate: The rate for the dropout layers, used to prevent overfitting.\n",
    "\n",
    "CustomSchedule: This class implements a custom learning rate that changes during training. It follows a \"warmup and decay\" pattern:\n",
    "\n",
    "    Warmup: For the first warmup_steps (e.g., 4000 steps), the learning rate starts small and increases linearly. This helps the model stabilize at \n",
    "    the beginning of training without making drastic, potentially damaging updates.\n",
    "\n",
    "    Decay: After the warmup phase, the learning rate decreases proportionally to the inverse square root of the step number. This allows for finer, \n",
    "    more precise adjustments as the model gets closer to a solution.\n",
    "\n",
    "Adam Optimizer: The Adam optimizer is used, but with specific beta_1, beta_2, and epsilon values that were found to work best for Transformers in the original paper.\n",
    "\n",
    "Compilation: Assembling the Model for Training\n",
    "\n",
    "The transformer.compile() step brings all the pieces together:\n",
    "\n",
    "    loss=masked_loss: It tells the model to use our custom masked_loss function, which correctly calculates the error while ignoring padding tokens.\n",
    "\n",
    "    optimizer=optimizer: It assigns the Adam optimizer with our custom learning rate schedule.\n",
    "\n",
    "    metrics=[masked_accuracy]: It tells the model to track and report the masked_accuracy during training, which gives a true measure of performance on the real words.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71090367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "# Just for demo, ideally these values are sub-optimal for transformer\n",
    "# Refer to the Attention paper\n",
    "\n",
    "\n",
    "# The number of stacked encoder or decoder layers. \n",
    "# More layers allow the model to learn more complex functions.\n",
    "num_layers = 4 \n",
    "\n",
    "# The dimensionality of the input and output vectors for the model. \n",
    "# It's the size of the embedding vectors for each token.\n",
    "d_model = 128  \n",
    "\n",
    "# The dimensionality of the inner \"feed-forward\" layer.\n",
    "# It's a standard practice to set this to 4 * d_model.\n",
    "dff = 512\n",
    "\n",
    "# The number of attention heads in the multi-head attention mechanism.\n",
    "# d_model must be divisible by num_heads.\n",
    "num_heads = 8\n",
    "\n",
    "# The dropout rate, used for regularization to prevent overfitting.\n",
    "# A value of 0.1 means 10% of neurons are randomly dropped during training.\n",
    "dropout_rate = 0.1\n",
    "\n",
    "# Create the Transformer model\n",
    "transformer = Transformer(\n",
    "    num_layers=num_layers,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    dff=dff,\n",
    "    input_vocab_size=tokenizers.pt.get_vocab_size().numpy(),\n",
    "    target_vocab_size=tokenizers.en.get_vocab_size().numpy(),\n",
    "    dropout_rate=dropout_rate)\n",
    "\n",
    "# Custom learning rate schedule\n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, dtype=tf.float32)\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "learning_rate = CustomSchedule(d_model)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n",
    "                                     epsilon=1e-9)\n",
    "\n",
    "# Compile the model\n",
    "transformer.compile(\n",
    "    loss=masked_loss,\n",
    "    optimizer=optimizer,\n",
    "    metrics=[masked_accuracy])\n",
    "\n",
    "# Train the model\n",
    "transformer.fit(train_batches,\n",
    "                epochs=10,\n",
    "                validation_data=val_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2320b207",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Original paper: Attention is all you need\n",
    "The models developed in this papers are given below. The demo is just a smaller version of it.\n",
    "\n",
    "1. Base Model\n",
    "\n",
    "This was the main architecture discussed and used for most experiments.\n",
    "\n",
    "    Number of Layers (N): 6 identical layers in both the encoder and decoder stacks.\n",
    "\n",
    "    Model Dimension (d_model): 512 for the embeddings and all sub-layer outputs.\n",
    "\n",
    "    Number of Attention Heads (h): 8 parallel attention heads.\n",
    "\n",
    "    Feed-Forward Layer Dimension (d_ff): 2048 for the inner-layer of the feed-forward network.\n",
    "\n",
    "    Dropout Rate: 0.1 was applied to the output of each sub-layer before it was added to the sub-layer input.\n",
    "\n",
    "2. Big Model\n",
    "\n",
    "This larger version was used to achieve the state-of-the-art results on the WMT-14 English-to-German translation task.\n",
    "\n",
    "    Number of Layers (N): 6 (This remained the same as the base model).\n",
    "\n",
    "    Model Dimension (d_model): 1024\n",
    "\n",
    "    Number of Attention Heads (h): 16\n",
    "\n",
    "    Feed-Forward Layer Dimension (d_ff): 4096\n",
    "\n",
    "    Dropout Rate: Increased to 0.3 for this specific translation task.\n",
    "\n",
    "3. Training Parameters\n",
    "\n",
    "Across both models, the training setup included:\n",
    "\n",
    "    Optimizer: They used the Adam optimizer with β₁ = 0.9, β₂ = 0.98, and ε = 10⁻⁹.\n",
    "\n",
    "    Learning Rate: A custom learning rate scheduler was used, which increased the rate linearly for the first 4000 warm-up steps and \n",
    "    then decreased it proportionally to the inverse square root of the step number.\n",
    "\n",
    "\n",
    " '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de877e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "102/102 [==============================] - 1790s 18s/step - loss: 4.0137 - masked_accuracy: 0.3586 - val_loss: 3.8566 - val_masked_accuracy: 0.3786\n",
      "Epoch 2/10\n",
      "102/102 [==============================] - 1796s 18s/step - loss: 3.8117 - masked_accuracy: 0.3803 - val_loss: 3.6596 - val_masked_accuracy: 0.3987\n",
      "Epoch 3/10\n",
      "102/102 [==============================] - 1768s 17s/step - loss: 3.6271 - masked_accuracy: 0.4003 - val_loss: 3.4967 - val_masked_accuracy: 0.4281\n",
      "Epoch 4/10\n",
      "102/102 [==============================] - 1742s 17s/step - loss: 3.4471 - masked_accuracy: 0.4207 - val_loss: 3.3345 - val_masked_accuracy: 0.4508\n",
      "Epoch 5/10\n",
      "102/102 [==============================] - 1769s 17s/step - loss: 3.2767 - masked_accuracy: 0.4395 - val_loss: 3.2603 - val_masked_accuracy: 0.4602\n",
      "Epoch 6/10\n",
      "102/102 [==============================] - 1776s 17s/step - loss: 3.1119 - masked_accuracy: 0.4580 - val_loss: 3.1395 - val_masked_accuracy: 0.4661\n",
      "Epoch 7/10\n",
      "102/102 [==============================] - 1798s 18s/step - loss: 2.9638 - masked_accuracy: 0.4743 - val_loss: 3.0047 - val_masked_accuracy: 0.4857\n",
      "Epoch 8/10\n",
      "102/102 [==============================] - 1788s 18s/step - loss: 2.8252 - masked_accuracy: 0.4909 - val_loss: 2.8830 - val_masked_accuracy: 0.4970\n",
      "Epoch 9/10\n",
      "102/102 [==============================] - 1782s 17s/step - loss: 2.7019 - masked_accuracy: 0.5047 - val_loss: 2.7773 - val_masked_accuracy: 0.5187\n",
      "Epoch 10/10\n",
      "102/102 [==============================] - 1811s 18s/step - loss: 2.5727 - masked_accuracy: 0.5207 - val_loss: 2.7022 - val_masked_accuracy: 0.5265\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fdc4c3fa530>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "transformer.fit(train_batches,\n",
    "                epochs=10,\n",
    "                validation_data=val_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ddb24a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "102/102 [==============================] - 1776s 17s/step - loss: 2.4670 - masked_accuracy: 0.5323 - val_loss: 2.6231 - val_masked_accuracy: 0.5407\n",
      "Epoch 2/20\n",
      "102/102 [==============================] - 1760s 17s/step - loss: 2.3617 - masked_accuracy: 0.5459 - val_loss: 2.6198 - val_masked_accuracy: 0.5353\n",
      "Epoch 3/20\n",
      "102/102 [==============================] - 1797s 18s/step - loss: 2.2737 - masked_accuracy: 0.5569 - val_loss: 2.5524 - val_masked_accuracy: 0.5409\n",
      "Epoch 4/20\n",
      "102/102 [==============================] - 1740s 17s/step - loss: 2.1913 - masked_accuracy: 0.5673 - val_loss: 2.5064 - val_masked_accuracy: 0.5551\n",
      "Epoch 5/20\n",
      "102/102 [==============================] - 1752s 17s/step - loss: 2.1194 - masked_accuracy: 0.5766 - val_loss: 2.5278 - val_masked_accuracy: 0.5523\n",
      "Epoch 6/20\n",
      "102/102 [==============================] - 1758s 17s/step - loss: 2.0473 - masked_accuracy: 0.5854 - val_loss: 2.4316 - val_masked_accuracy: 0.5597\n",
      "Epoch 7/20\n",
      "102/102 [==============================] - 1749s 17s/step - loss: 1.9858 - masked_accuracy: 0.5940 - val_loss: 2.3935 - val_masked_accuracy: 0.5620\n",
      "Epoch 8/20\n",
      "102/102 [==============================] - 1761s 17s/step - loss: 1.9199 - masked_accuracy: 0.6029 - val_loss: 2.4014 - val_masked_accuracy: 0.5619\n",
      "Epoch 9/20\n",
      "102/102 [==============================] - 1793s 18s/step - loss: 1.8576 - masked_accuracy: 0.6119 - val_loss: 2.4687 - val_masked_accuracy: 0.5637\n",
      "Epoch 10/20\n",
      "102/102 [==============================] - 1780s 17s/step - loss: 1.8095 - masked_accuracy: 0.6184 - val_loss: 2.3558 - val_masked_accuracy: 0.5774\n",
      "Epoch 11/20\n",
      "102/102 [==============================] - 1775s 17s/step - loss: 1.7558 - masked_accuracy: 0.6265 - val_loss: 2.3518 - val_masked_accuracy: 0.5795\n",
      "Epoch 12/20\n",
      "102/102 [==============================] - 1780s 17s/step - loss: 1.7051 - masked_accuracy: 0.6337 - val_loss: 2.3721 - val_masked_accuracy: 0.5807\n",
      "Epoch 13/20\n",
      "102/102 [==============================] - 1800s 18s/step - loss: 1.6603 - masked_accuracy: 0.6405 - val_loss: 2.3138 - val_masked_accuracy: 0.5919\n",
      "Epoch 14/20\n",
      "102/102 [==============================] - 1762s 17s/step - loss: 1.6143 - masked_accuracy: 0.6476 - val_loss: 2.3437 - val_masked_accuracy: 0.5837\n",
      "Epoch 15/20\n",
      "102/102 [==============================] - 1753s 17s/step - loss: 1.5755 - masked_accuracy: 0.6529 - val_loss: 2.3838 - val_masked_accuracy: 0.5775\n",
      "Epoch 16/20\n",
      "102/102 [==============================] - 1746s 17s/step - loss: 1.5354 - masked_accuracy: 0.6590 - val_loss: 2.2974 - val_masked_accuracy: 0.5921\n",
      "Epoch 17/20\n",
      "102/102 [==============================] - 1735s 17s/step - loss: 1.5001 - masked_accuracy: 0.6650 - val_loss: 2.2927 - val_masked_accuracy: 0.5917\n",
      "Epoch 18/20\n",
      "102/102 [==============================] - 1744s 17s/step - loss: 1.4623 - masked_accuracy: 0.6702 - val_loss: 2.2989 - val_masked_accuracy: 0.5948\n",
      "Epoch 19/20\n",
      "102/102 [==============================] - 1739s 17s/step - loss: 1.4378 - masked_accuracy: 0.6739 - val_loss: 2.2935 - val_masked_accuracy: 0.5920\n",
      "Epoch 20/20\n",
      "102/102 [==============================] - 1765s 17s/step - loss: 1.4014 - masked_accuracy: 0.6798 - val_loss: 2.2915 - val_masked_accuracy: 0.6026\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fdc4c3f9990>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "transformer.fit(train_batches,\n",
    "                epochs=20,\n",
    "                validation_data=val_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5a72fb85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "102/102 [==============================] - 1736s 17s/step - loss: 1.3648 - masked_accuracy: 0.6855 - val_loss: 2.3176 - val_masked_accuracy: 0.6015\n",
      "Epoch 2/12\n",
      "102/102 [==============================] - 1710s 17s/step - loss: 1.3286 - masked_accuracy: 0.6917 - val_loss: 2.2939 - val_masked_accuracy: 0.6002\n",
      "Epoch 3/12\n",
      "102/102 [==============================] - 1750s 17s/step - loss: 1.2880 - masked_accuracy: 0.6985 - val_loss: 2.2975 - val_masked_accuracy: 0.6031\n",
      "Epoch 4/12\n",
      "102/102 [==============================] - 1746s 17s/step - loss: 1.2534 - masked_accuracy: 0.7042 - val_loss: 2.3159 - val_masked_accuracy: 0.6020\n",
      "Epoch 5/12\n",
      "102/102 [==============================] - 1751s 17s/step - loss: 1.2224 - masked_accuracy: 0.7095 - val_loss: 2.3651 - val_masked_accuracy: 0.5976\n",
      "Epoch 6/12\n",
      "102/102 [==============================] - 1734s 17s/step - loss: 1.1930 - masked_accuracy: 0.7148 - val_loss: 2.3102 - val_masked_accuracy: 0.6031\n",
      "Epoch 7/12\n",
      "102/102 [==============================] - 1727s 17s/step - loss: 1.1609 - masked_accuracy: 0.7204 - val_loss: 2.3320 - val_masked_accuracy: 0.6068\n",
      "Epoch 8/12\n",
      "102/102 [==============================] - 1739s 17s/step - loss: 1.1317 - masked_accuracy: 0.7245 - val_loss: 2.3522 - val_masked_accuracy: 0.5998\n",
      "Epoch 9/12\n",
      "102/102 [==============================] - 1730s 17s/step - loss: 1.1091 - masked_accuracy: 0.7291 - val_loss: 2.3581 - val_masked_accuracy: 0.6094\n",
      "Epoch 10/12\n",
      "102/102 [==============================] - 1740s 17s/step - loss: 1.0817 - masked_accuracy: 0.7341 - val_loss: 2.3628 - val_masked_accuracy: 0.6128\n",
      "Epoch 11/12\n",
      "102/102 [==============================] - 1751s 17s/step - loss: 1.0567 - masked_accuracy: 0.7382 - val_loss: 2.3857 - val_masked_accuracy: 0.6074\n",
      "Epoch 12/12\n",
      "102/102 [==============================] - 1752s 17s/step - loss: 1.0360 - masked_accuracy: 0.7423 - val_loss: 2.3756 - val_masked_accuracy: 0.6114\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fdc4c2f6890>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "transformer.fit(train_batches,\n",
    "                epochs=12,\n",
    "                validation_data=val_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ce8a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Step 13: Define the transformer evaluation model\n",
    "\n",
    "This `Translator` class BELOW is carefully designed to handle the inference with a Transformer model inside a performant `tf.function`. \n",
    "Here’s a breakdown of the key concepts:\n",
    "\n",
    "1. The Autoregressive Loop\n",
    "The core of translation is the `for` loop. This process is called autoregressive decoding because the model's prediction at each step \n",
    "is fed back into itself to generate the next prediction.\n",
    "\n",
    "Why it's needed: Language is sequential. To predict the word \"cat,\" you need to know that the previous words were \"the black...\". \n",
    "The loop mimics this by building the output sequence one token at a time, using its own previous predictions to inform the next one.\n",
    "How it works\n",
    "    1.  Start:The decoder is given the `[START]` token.\n",
    "    2.  Predict: The model predicts the first word (e.g., \"this\").\n",
    "    3.  Append: The decoder's input is now `[START]`, \"this\".\n",
    "    4.  Predict: The model predicts the second word (e.g., \"is\").\n",
    "    5.  Repeat: The input becomes `[START]`, \"this\", \"is\", and so on, until the model predicts the `[END]` token or hits the `max_length`.\n",
    "\n",
    "2. `tf.function` and `tf.TensorArray`\n",
    "Why it's needed: Running a loop in standard Python (\"eager mode\") is very slow because each step involves communication between \n",
    "Python and the TensorFlow backend. The `@tf.function` decorator traces the Python code and compiles it into a highly optimized,\n",
    " static TensorFlow graph that runs much faster.\n",
    "\n",
    "The Challenge: A standard Python list (`my_list = []`) cannot be modified inside a compiled graph loop. `tf.TensorArray` is the \n",
    "TensorFlow-native equivalent that is designed specifically for this purpose. It allows you to dynamically build up a tensor inside a graph loop.\n",
    "\n",
    "3. Recalculating Attention Weights (The `InaccessibleTensorError` Fix)\n",
    "This is the most complex but important part of the code.\n",
    "\n",
    "Why it's needed: The `@tf.function` turns the `for` loop into a `tf.while_loop` operation in its graph. This `while_loop` has its own internal **scope**. \n",
    "Any tensor created inside that scope (like the `attention_weights` at each step) is temporary and cannot be accessed from outside the loop once it finishes. \n",
    "Trying to access `self.transformer.decoder.last_attn_scores` after the loop would be pointing to a tensor that no longer exists in the graph's main scope, \n",
    "causing the `InaccessibleTensorError`.\n",
    "The Solution: The fix is to run the model one more time after the loop is complete.\n",
    "    1.  We take the final, complete `output` sequence that was generated.\n",
    "    2.  We perform a full forward pass with this `output`.\n",
    "    3.  Since this forward pass happens outside the `while_loop`'s scope, the `attention_weights` it generates are now accessible in the main function's scope and can be safely returned.\n",
    "    4.  We use `output[:, :-1]` as the decoder input because which can allow us to see the attention the model was paying when it was about to predict the final tok\n",
    "\n",
    " '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d3215fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Translator(tf.Module):\n",
    "  \"\"\"\n",
    "  A tf.Module that encapsulates the trained Transformer model for inference.\n",
    "  It provides a clean interface to translate a sentence from Portuguese to English.\n",
    "  \"\"\"\n",
    "  def __init__(self, tokenizers, transformer):\n",
    "    \"\"\"\n",
    "    Initializes the Translator.\n",
    "\n",
    "    Args:\n",
    "      tokenizers: The loaded tokenizers object containing .pt and .en sub-models.\n",
    "      transformer: The trained and compiled Transformer Keras model.\n",
    "    \"\"\"\n",
    "    self.tokenizers = tokenizers\n",
    "    self.transformer = transformer\n",
    "\n",
    "  # This decorator compiles the Python function into a high-performance,\n",
    "  # static TensorFlow graph. This is crucial for speed during inference.\n",
    "  # Without it, the loop would run in slow Python \"eager mode\".\n",
    "  @tf.function\n",
    "  def __call__(self, sentence, max_length=MAX_TOKENS):\n",
    "    \"\"\"\n",
    "    The main translation method. It takes a raw sentence and performs\n",
    "    autoregressive decoding to generate the translation.\n",
    "\n",
    "    Args:\n",
    "      sentence: A scalar tf.Tensor of type string (the Portuguese sentence).\n",
    "      max_length: The maximum number of tokens to generate for the translation.\n",
    "    \"\"\"\n",
    "    # --- 1. PREPARE THE INPUTS ---\n",
    "\n",
    "    # The model expects a batch of sentences, so we add a batch dimension\n",
    "    # to the single input sentence. Shape: () -> (1,).\n",
    "    assert isinstance(sentence, tf.Tensor)\n",
    "    if len(sentence.shape) == 0:\n",
    "      sentence = sentence[tf.newaxis]\n",
    "\n",
    "    # Tokenize the Portuguese input sentence, converting it from text to a\n",
    "    # sequence of integer IDs. The `.to_tensor()` call pads the batch.\n",
    "    # This becomes the input to the Encoder.\n",
    "    encoder_input = self.tokenizers.pt.tokenize(sentence).to_tensor()\n",
    "\n",
    "    # --- 2. INITIALIZE THE DECODER'S INPUT ---\n",
    "\n",
    "    # Get the special [START] and [END] token IDs from the English tokenizer.\n",
    "    # These are essential for controlling the generation process.\n",
    "    start_end = self.tokenizers.en.tokenize([''])[0]\n",
    "    start = start_end[0][tf.newaxis] # The [START] token ID.\n",
    "    end = start_end[1][tf.newaxis]   # The [END] token ID.\n",
    "\n",
    "    # The decoder's input sequence starts with only the [START] token.\n",
    "    # We use tf.TensorArray to build the output sequence dynamically.\n",
    "    # This is a special TensorFlow-aware data structure that can be written to\n",
    "    # inside a tf.function's loop. A standard Python list would not work here.\n",
    "    output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)\n",
    "    output_array = output_array.write(0, start)\n",
    "\n",
    "    # --- 3. THE AUTOREGRESSIVE DECODING LOOP ---\n",
    "\n",
    "    # This loop generates the translation one token at a time.\n",
    "    for i in tf.range(max_length):\n",
    "      # Get the sequence generated so far and prepare it as the decoder's input.\n",
    "      output = tf.transpose(output_array.stack())\n",
    "\n",
    "      # Run a forward pass through the entire Transformer model.\n",
    "      # The encoder processes the Portuguese sentence, and the decoder uses that\n",
    "      # context along with the English translation generated so far (`output`).\n",
    "      predictions = self.transformer([encoder_input, output], training=False)\n",
    "\n",
    "      # We only care about the prediction for the very last token in the sequence.\n",
    "      # Shape: (batch_size, seq_len, vocab_size) -> (batch_size, 1, vocab_size).\n",
    "      predictions = predictions[:, -1:, :]\n",
    "\n",
    "      # Select the token with the highest probability (logit score).\n",
    "      # This is a \"greedy search\" decoding strategy.\n",
    "      predicted_id = tf.argmax(predictions, axis=-1)\n",
    "\n",
    "      # Write the newly predicted token ID to our output array. This token will\n",
    "      # be part of the decoder's input in the next iteration.\n",
    "      output_array = output_array.write(i+1, predicted_id[0])\n",
    "\n",
    "      # If the model predicts the [END] token, we can stop generating.\n",
    "      if predicted_id == end:\n",
    "        break\n",
    "\n",
    "    # --- 4. FINALIZE OUTPUTS ---\n",
    "\n",
    "    # Stack all the generated token IDs into a final tensor.\n",
    "    output = tf.transpose(output_array.stack())\n",
    "    # Convert the token IDs back into a human-readable text string.\n",
    "    text = self.tokenizers.en.detokenize(output)[0]\n",
    "\n",
    "    # Also, convert the token IDs into their string representations for inspection.\n",
    "    tokens = self.tokenizers.en.lookup(output)[0]\n",
    "\n",
    "    # --- 5. RECALCULATE ATTENTION WEIGHTS (CRITICAL FIX) ---\n",
    "\n",
    "    # Because this function is a @tf.function, tensors created inside the `for`\n",
    "    # loop (which becomes a `tf.while_loop`) are in a different graph scope\n",
    "    # and cannot be accessed after the loop finishes.\n",
    "    # Therefore, we must run the model one final time *outside* the loop\n",
    "    # with the complete generated sequence to get the final attention weights.\n",
    "    # We pass `output[:, :-1]` as the decoder input because the attention weights\n",
    "    # are calculated based on what the model used to predict the *next* token.\n",
    "    self.transformer([encoder_input, output[:,:-1]], training=False)\n",
    "    attention_weights = self.transformer.decoder.last_attn_scores\n",
    "\n",
    "    return text, tokens, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "33cedcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = Translator(tokenizers, transformer)\n",
    "\n",
    "def print_translation(sentence, tokens, ground_truth):\n",
    "  print(f'{\"Input:\":15s}: {sentence}')\n",
    "  print(f'{\"Prediction\":15s}: {tokens.numpy().decode(\"utf-8\")}')\n",
    "  print(f'{\"Ground truth\":15s}: {ground_truth}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8352f607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:         : este é um problema que temos que resolver.\n",
      "Prediction     : this is a problem that we have to solve .\n",
      "Ground truth   : this is a problem we have to solve .\n"
     ]
    }
   ],
   "source": [
    "sentence = 'este é um problema que temos que resolver.'\n",
    "ground_truth = 'this is a problem we have to solve .'\n",
    "\n",
    "translated_text, translated_tokens, attention_weights = translator(\n",
    "    tf.constant(sentence))\n",
    "print_translation(sentence, translated_text, ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "73c8f39f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:         : os meus vizinhos ouviram sobre esta ideia.\n",
      "Prediction     : my neighbors have heard about this idea .\n",
      "Ground truth   : and my neighboring homes heard about this idea .\n"
     ]
    }
   ],
   "source": [
    "sentence = 'os meus vizinhos ouviram sobre esta ideia.'\n",
    "ground_truth = 'and my neighboring homes heard about this idea .'\n",
    "\n",
    "translated_text, translated_tokens, attention_weights = translator(\n",
    "    tf.constant(sentence))\n",
    "print_translation(sentence, translated_text, ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "635dd14c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:         : vou então muito rapidamente partilhar convosco algumas histórias de algumas coisas mágicas que aconteceram.\n",
      "Prediction     : so i ' m going to very quickly share with you some stories of some magical things that happened .\n",
      "Ground truth   : so i'll just share with you some stories very quickly of some magical things that have happened.\n"
     ]
    }
   ],
   "source": [
    "sentence = 'vou então muito rapidamente partilhar convosco algumas histórias de algumas coisas mágicas que aconteceram.'\n",
    "ground_truth = \"so i'll just share with you some stories very quickly of some magical things that have happened.\"\n",
    "\n",
    "translated_text, translated_tokens, attention_weights = translator(\n",
    "    tf.constant(sentence))\n",
    "print_translation(sentence, translated_text, ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9048e3df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as positional_embedding_layer_call_fn, positional_embedding_layer_call_and_return_conditional_losses, dropout_4_layer_call_fn, dropout_4_layer_call_and_return_conditional_losses, positional_embedding_1_layer_call_fn while saving (showing 5 of 316). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: translator/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: translator/assets\n"
     ]
    }
   ],
   "source": [
    "tf.saved_model.save(translator, export_dir='translator')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fcd032",
   "metadata": {},
   "source": [
    "Use the Transformer saved model for inference"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
