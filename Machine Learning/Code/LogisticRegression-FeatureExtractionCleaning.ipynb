{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08b2c5c7",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "This notebook implements a **Logistic Regression** model from scratch to predict passenger survival on the Titanic. The process includes:\n",
    "1. Loading the dataset.\n",
    "2. Cleaning and preprocessing the data (handling missing values and categorical features).\n",
    "3. Training a custom logistic regression model using gradient descent.\n",
    "4. Evaluating the model's accuracy.\n",
    "5. Comparing its performance to the standard `scikit-learn` implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09d3db15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86d7886",
   "metadata": {},
   "source": [
    "## 1. Core Model Functions\n",
    "\n",
    "Here we define all the functions required for our custom Logistic Regression model. This includes the sigmoid activation function, the gradient calculation, the training loop, and the prediction function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c486742",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(x, y, epochs, learning_rate):\n",
    "    \"\"\"Trains the logistic regression model using gradient descent.\"\"\"\n",
    "    weights = np.zeros(x.shape[1])\n",
    "    bias = 0\n",
    "    for i in range(epochs):\n",
    "        x_dot_weights = np.matmul(weights, x.transpose()) + bias\n",
    "        pred = sigmoid(x_dot_weights)\n",
    "        error_w, error_b = compute_gradients(x, y, pred)\n",
    "        weights -= learning_rate * error_w\n",
    "        bias -= learning_rate * error_b\n",
    "    return weights, bias\n",
    "\n",
    "def compute_gradients(x, y_true, y_pred):\n",
    "    \"\"\"Computes the gradients of the binary cross-entropy loss function.\"\"\"\n",
    "    difference = y_pred - y_true\n",
    "    gradient_b = np.mean(difference)\n",
    "    gradients_w = np.matmul(x.transpose(), difference) / len(y_true)\n",
    "    return gradients_w, gradient_b\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"Element-wise wrapper for the sigmoid function.\"\"\"\n",
    "    return np.array([sigmoid_function(value) for value in x])\n",
    "\n",
    "def sigmoid_function(x):\n",
    "    \"\"\"Numerically stable implementation of the sigmoid function.\"\"\"\n",
    "    if x >= 0:\n",
    "        z = np.exp(-x)\n",
    "        return 1 / (1 + z)\n",
    "    else:\n",
    "        z = np.exp(x)\n",
    "        return z / (1 + z)\n",
    "\n",
    "def predict(x, weights, bias):\n",
    "    \"\"\"Uses the learned weights and bias to make predictions.\"\"\"\n",
    "    x_dot_weights = np.matmul(weights, x.transpose()) + bias\n",
    "    probabilities = sigmoid(x_dot_weights)\n",
    "    return [1 if p > 0.5 else 0 for p in probabilities]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a7a4e2",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Preprocessing\n",
    "\n",
    "This is a critical step where we load the raw Titanic dataset and clean it up. We will:\n",
    "- Convert categorical features like `Sex` and `Embarked` into numerical values.\n",
    "- Handle missing data, specifically in the `Age` column, by filling `NaN` values with the mean age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc3cc8ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'Titanic-Dataset.csv' loaded successfully!\n",
      "\n",
      "Data cleaning complete. First 5 rows of processed features (x):\n",
      "    Age  Sex  Pclass     Fare  Embarked\n",
      "0  22.0  1.0     3.0   7.2500       0.0\n",
      "1  38.0  0.0     1.0  71.2833       1.0\n",
      "2  26.0  0.0     3.0   7.9250       0.0\n",
      "3  35.0  0.0     1.0  53.1000       0.0\n",
      "4  35.0  1.0     3.0   8.0500       0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raman Tayal\\AppData\\Local\\Temp\\ipykernel_18892\\1916656791.py:14: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  features_df['Age'].fillna(features_df['Age'].mean(), inplace=True)\n",
      "C:\\Users\\Raman Tayal\\AppData\\Local\\Temp\\ipykernel_18892\\1916656791.py:21: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  features_df['Embarked'].fillna('S', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Set the path to your CSV file.\n",
    "path = r\"Titanic-Dataset.csv\"\n",
    "\n",
    "try:\n",
    "    file = pd.read_csv(path)\n",
    "    print(f\"File '{path}' loaded successfully!\")\n",
    "\n",
    "    # --- Feature Extraction and Cleaning ---\n",
    "    y = file[\"Survived\"].values\n",
    "    # Create a copy to avoid SettingWithCopyWarning\n",
    "    features_df = file[[\"Age\", \"Sex\", \"Pclass\", \"Fare\", \"Embarked\"]].copy()\n",
    "    \n",
    "    # 1. Fill missing Age values with the mean\n",
    "    features_df['Age'].fillna(features_df['Age'].mean(), inplace=True)\n",
    "    \n",
    "    # 2. Convert 'Sex' to numerical (male: 1, female: 0)\n",
    "    features_df['Sex'] = features_df['Sex'].map({'male': 1, 'female': 0})\n",
    "\n",
    "    # 3. Convert 'Embarked' to numerical (S: 0, C: 1, Q: 2)\n",
    "    # First, fill any missing embarked values with the most common port ('S')\n",
    "    features_df['Embarked'].fillna('S', inplace=True)\n",
    "    features_df['Embarked'] = features_df['Embarked'].map({'S': 0, 'C': 1, 'Q': 2})\n",
    "\n",
    "    # Create the final feature matrix `x`\n",
    "    x = features_df.values\n",
    "\n",
    "    print(\"\\nData cleaning complete. First 5 rows of processed features (x):\")\n",
    "    print(pd.DataFrame(x, columns=features_df.columns).head())\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file was not found at '{path}'\")\n",
    "    print(\"Please make sure 'Titanic-Dataset.csv' is in the same directory as the notebook.\")\n",
    "    x, y = None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Splitting Data for Training and Testing\n",
    "\n",
    "We'll split our processed data into a training set and a testing set. The model will learn from the training set, and we'll evaluate its performance on the unseen testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data split complete:\n",
      "x_train shape: (712, 5)\n",
      "x_test shape:  (179, 5)\n"
     ]
    }
   ],
   "source": [
    "if x is not None:\n",
    "    # A common 80/20 split is used for training and testing.\n",
    "    # random_state ensures that the split is the same every time we run the code.\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.8, random_state=42)\n",
    "\n",
    "    print(\"Data split complete:\")\n",
    "    print(f\"x_train shape: {x_train.shape}\")\n",
    "    print(f\"x_test shape:  {x_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training and Evaluating Our Custom Model\n",
    "\n",
    "Now we train our custom model using the training data and then evaluate its performance by making predictions on the test set and calculating the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Custom Logistic Regression Model ---\n",
      "Learned Weights: [-0.0272077  -0.01265766 -0.01469868  0.0128648   0.00156715]\n",
      "Learned Bias: -0.002478327364715696\n",
      "Our model accuracy: 0.6536\n"
     ]
    }
   ],
   "source": [
    "if x is not None:\n",
    "    # Train the model and get the learned parameters\n",
    "    weights, bias = train(x_train, y_train, epochs=1000, learning_rate=0.0001)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = predict(x_test, weights, bias)\n",
    "\n",
    "    print(\"--- Custom Logistic Regression Model ---\")\n",
    "    print(f\"Learned Weights: {weights}\")\n",
    "    print(f\"Learned Bias: {bias}\")\n",
    "    print(f\"Our model accuracy: {accuracy_score(y_test, y_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparison with Scikit-learn\n",
    "\n",
    "Finally, we train a `scikit-learn` `LogisticRegression` model on the exact same data to see how our implementation compares to a standard, highly optimized library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Scikit-learn Logistic Regression Model ---\n",
      "Sklearn model accuracy: 0.7933\n"
     ]
    }
   ],
   "source": [
    "if x is not None:\n",
    "    # Create an instance of the sklearn model\n",
    "    model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(x_train, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    sklearn_pred = model.predict(x_test)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    sklearn_accuracy = accuracy_score(y_test, sklearn_pred)\n",
    "\n",
    "    print(\"--- Scikit-learn Logistic Regression Model ---\")\n",
    "    print(f\"Sklearn model accuracy: {sklearn_accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
