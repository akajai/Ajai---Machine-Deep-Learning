{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam Detection using Naive Bayes\n",
    "\n",
    "This notebook implements a **Bernoulli Naive Bayes** classifier from scratch to distinguish spam emails from non-spam (ham) emails. We will use the [Spambase Data Set](https://archive.ics.uci.edu/ml/datasets/spambase) from the UCI Machine Learning Repository. üìß\n",
    "\n",
    "The core idea is to calculate the probability of an email being spam given its features, and compare that to the probability of it being not spam. According to Bayes' theorem:\n",
    "\n",
    "$$ P(\\text{Class} | \\text{Features}) = \\frac{P(\\text{Features} | \\text{Class}) \\cdot P(\\text{Class})}{P(\\text{Features})} $$\n",
    "\n",
    "Since we only care about which class has a higher probability, we can ignore the denominator $P(\\text{Features})$ and compare the posteriors:\n",
    "\n",
    "$$ \\text{posterior} \\propto \\text{likelihood} \\times \\text{prior} $$\n",
    "\n",
    "To avoid numerical underflow (multiplying many small probabilities results in a number too small for the computer to store), we work with the log of the probabilities:\n",
    "\n",
    "$$ \\log(\\text{posterior}) = \\log(\\text{likelihood}) + \\log(\\text{prior}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing Necessary Libraries\n",
    "\n",
    "We'll start by importing `numpy` for numerical operations and `train_test_split` from `scikit-learn` to divide our data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Defining the Classifier Functions\n",
    "\n",
    "Here we define the core functions that will form our Naive Bayes classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_log_likelihoods_with_naive_bayes(feature_vector, Class):\n",
    "    \"\"\"\n",
    "    Calculates the log-likelihood of a feature vector for a given class.\n",
    "    This is log(P(Features | Class)).\n",
    "    \n",
    "    Args:\n",
    "        feature_vector (list): A binarized vector where 1 means a feature is present, 0 otherwise.\n",
    "        Class (int): The class to calculate the likelihood for (0 for 'ham', 1 for 'spam').\n",
    "        \n",
    "    Returns:\n",
    "        float: The total log-likelihood.\n",
    "    \"\"\"\n",
    "    # Ensure the feature vector has the correct number of features.\n",
    "    assert len(feature_vector) == num_features\n",
    "    \n",
    "    # Initialize log-likelihood to 0.0. We will add to this value.\n",
    "    log_likelihood = 0.0\n",
    "    \n",
    "    # If the class is 'ham' (0)\n",
    "    if Class == 0:\n",
    "        # Iterate through each feature and its status (present/absent)\n",
    "        for feature_index in range(len(feature_vector)):\n",
    "            # If the feature is present in the email (value is 1)\n",
    "            if feature_vector[feature_index] == 1:\n",
    "                # Add the log-likelihood of this feature being PRESENT given class 0\n",
    "                log_likelihood += np.log10(likelihoods_class_0[feature_index])\n",
    "            # If the feature is absent from the email (value is 0)\n",
    "            elif feature_vector[feature_index] == 0:\n",
    "                # Add the log-likelihood of this feature being ABSENT given class 0\n",
    "                # This is log(1 - P(feature=1 | class=0))\n",
    "                log_likelihood += np.log10(1.0 - likelihoods_class_0[feature_index])\n",
    "    \n",
    "    # If the class is 'spam' (1)\n",
    "    elif Class == 1:\n",
    "        # Iterate through each feature and its status (present/absent)\n",
    "        for feature_index in range(len(feature_vector)):\n",
    "            # If the feature is present in the email (value is 1)\n",
    "            if feature_vector[feature_index] == 1:\n",
    "                # Add the log-likelihood of this feature being PRESENT given class 1\n",
    "                log_likelihood += np.log10(likelihoods_class_1[feature_index])\n",
    "            # If the feature is absent from the email (value is 0)\n",
    "            elif feature_vector[feature_index] == 0:\n",
    "                # Add the log-likelihood of this feature being ABSENT given class 1\n",
    "                # This is log(1 - P(feature=1 | class=1))\n",
    "                log_likelihood += np.log10(1.0 - likelihoods_class_1[feature_index])\n",
    "    else:\n",
    "        raise ValueError(\"Class takes integer values 0 or 1\")\n",
    "    \n",
    "    return log_likelihood\n",
    "\n",
    "\n",
    "def calculate_class_posteriors(feature_vector):\n",
    "    \"\"\"\n",
    "    Calculates the log posterior probability for each class.\n",
    "    This is log(P(Class | Features)) which is proportional to log(likelihood) + log(prior).\n",
    "    \n",
    "    Args:\n",
    "        feature_vector (list): A binarized feature vector.\n",
    "        \n",
    "    Returns:\n",
    "        tuple: A tuple containing the log posterior for class 0 and class 1.\n",
    "    \"\"\"\n",
    "    # Calculate log likelihoods for both classes using the function defined above.\n",
    "    log_likelihood_class_0 = calculate_log_likelihoods_with_naive_bayes(feature_vector, Class=0)\n",
    "    log_likelihood_class_1 = calculate_log_likelihoods_with_naive_bayes(feature_vector, Class=1)\n",
    "\n",
    "    # Calculate log posteriors by adding the log priors (calculated during training).\n",
    "    log_posterior_class_0 = log_likelihood_class_0 + log_prior_class_0\n",
    "    log_posterior_class_1 = log_likelihood_class_1 + log_prior_class_1\n",
    "\n",
    "    return log_posterior_class_0, log_posterior_class_1\n",
    "\n",
    "\n",
    "def classify_spam(document_vector):\n",
    "    \"\"\"\n",
    "    Classifies an email as spam or not based on its document vector.\n",
    "    \n",
    "    Args:\n",
    "        document_vector (list): The original feature vector from the dataset (contains frequencies).\n",
    "        \n",
    "    Returns:\n",
    "        int: The predicted class (0 for ham, 1 for spam).\n",
    "    \"\"\"\n",
    "    # For Bernoulli Naive Bayes, we only care about the presence or absence of a feature.\n",
    "    # We convert the frequency-based vector into a binary vector.\n",
    "    # Any feature with a frequency > 0.0 is considered 'present' (1).\n",
    "    feature_vector = [int(element > 0.0) for element in document_vector]\n",
    "    \n",
    "    # Calculate the log posteriors for this binarized feature vector.\n",
    "    log_posterior_class_0, log_posterior_class_1 = calculate_class_posteriors(feature_vector)\n",
    "    \n",
    "    # The class with the higher log posterior probability is our prediction.\n",
    "    if log_posterior_class_0 > log_posterior_class_1:\n",
    "        return 0  # Predict 'ham'\n",
    "    else:\n",
    "        return 1  # Predict 'spam'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Performance Evaluation Function\n",
    "\n",
    "This simple function calculates the accuracy of our model by comparing its predictions to the true labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_performance(predictions, ground_truth_labels):\n",
    "    \"\"\"\n",
    "    Calculates the accuracy of the classifier.\n",
    "    \n",
    "    Args:\n",
    "        predictions (list): A list of predicted labels (0 or 1).\n",
    "        ground_truth_labels (list): The list of true labels.\n",
    "        \n",
    "    Returns:\n",
    "        float: The accuracy as a value between 0 and 1.\n",
    "    \"\"\"\n",
    "    # Counter for correct predictions.\n",
    "    correct_count = 0.0\n",
    "    \n",
    "    # Iterate through all predictions.\n",
    "    for item_index in range(len(predictions)):\n",
    "        # If the prediction matches the true label, increment the counter.\n",
    "        if predictions[item_index] == ground_truth_labels[item_index]:\n",
    "            correct_count += 1.0\n",
    "            \n",
    "    # Accuracy is the ratio of correct predictions to the total number of predictions.\n",
    "    accuracy = correct_count / len(predictions)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Main Execution Block\n",
    "\n",
    "This is where the main logic resides. We'll load the data, process it, train the model, make predictions, and evaluate the results. ‚öôÔ∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the Naive Bayes classifier is: 0.8705\n"
     ]
    }
   ],
   "source": [
    "# Set a fixed file path. IMPORTANT: You must change this to the location of 'spambase.data' on your system.\n",
    "file_path = \"spambase.data\"\n",
    "\n",
    "# --- Data Loading ---\n",
    "try:\n",
    "    with open(file_path, 'r') as datafile:\n",
    "        data = []\n",
    "        # Read the file line by line.\n",
    "        for line in datafile:\n",
    "            # Each line is a comma-separated list of values. We split it and convert to floats.\n",
    "            line = [float(element) for element in line.strip().split(',')]\n",
    "            # Append the processed line as a numpy array to our data list.\n",
    "            data.append(np.asarray(line))\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file '{file_path}' was not found.\")\n",
    "    print(\"Please download 'spambase.data' and update the 'file_path' variable.\")\n",
    "    data = []\n",
    "\n",
    "if data:\n",
    "    # --- Data Preparation ---\n",
    "    # The first 48 columns are features (word frequencies).\n",
    "    num_features = 48\n",
    "\n",
    "    # Extract the feature vectors (first 48 columns) for all emails.\n",
    "    X = [data[i][:num_features] for i in range(len(data))]\n",
    "\n",
    "    # Extract the target labels (the last column), converting them to integers (0 or 1).\n",
    "    y = [int(data[i][-1]) for i in range(len(data))]\n",
    "\n",
    "    # --- Train-Test Split ---\n",
    "    # Split the dataset into a training set (25%) and a testing set (75%).\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.25, random_state=42)\n",
    "\n",
    "    # --- Model Training ---\n",
    "    # The \"training\" in Naive Bayes involves calculating prior probabilities and likelihoods from the training data.\n",
    "\n",
    "    # 1. Calculate Class Priors: P(Class)\n",
    "    # Separate the training data by class (ham vs. spam).\n",
    "    X_train_class_0 = [X_train[i] for i in range(len(X_train)) if y_train[i] == 0] # Ham emails\n",
    "    X_train_class_1 = [X_train[i] for i in range(len(X_train)) if y_train[i] == 1] # Spam emails\n",
    "\n",
    "    # Count the number of emails in each class.\n",
    "    num_class_0 = float(len(X_train_class_0))\n",
    "    num_class_1 = float(len(X_train_class_1))\n",
    "\n",
    "    # Prior probability is the ratio of emails in a class to the total number of emails.\n",
    "    prior_probability_class_0 = num_class_0 / (num_class_0 + num_class_1)\n",
    "    prior_probability_class_1 = num_class_1 / (num_class_0 + num_class_1)\n",
    "\n",
    "    # Calculate the log of the priors.\n",
    "    log_prior_class_0 = np.log10(prior_probability_class_0)\n",
    "    log_prior_class_1 = np.log10(prior_probability_class_1)\n",
    "\n",
    "    # 2. Calculate Likelihoods: P(Feature | Class)\n",
    "    # For Bernoulli Naive Bayes, we binarize the feature vectors first.\n",
    "    X_train_class_0_binary = np.array([np.array(x) > 0.0 for x in X_train_class_0])\n",
    "    X_train_class_1_binary = np.array([np.array(x) > 0.0 for x in X_train_class_1])\n",
    "\n",
    "    # The likelihood of a feature being present for a class is the mean of that feature's column in the binarized data.\n",
    "    # We add a small value (1e-6) for Laplace smoothing to avoid log(0) errors if a feature never appears in a class.\n",
    "    likelihoods_class_0 = np.mean(X_train_class_0_binary, axis=0) + 1e-6\n",
    "    likelihoods_class_1 = np.mean(X_train_class_1_binary, axis=0) + 1e-6\n",
    "\n",
    "    # --- Prediction ---\n",
    "    # Make predictions on the unseen test set.\n",
    "    predictions = []\n",
    "    for email in X_test:\n",
    "        predictions.append(classify_spam(email))\n",
    "\n",
    "    # --- Evaluation ---\n",
    "    # Calculate and print the accuracy of the model.\n",
    "    accuracy_of_naive_bayes = evaluate_performance(predictions, y_test)\n",
    "    print(f\"The accuracy of the Naive Bayes classifier is: {accuracy_of_naive_bayes:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
